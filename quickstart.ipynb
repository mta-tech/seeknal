{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "import os\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark  # noqa\n",
    "from pyspark.sql import SparkSession  # noqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_dir = os.path.dirname(os.path.realpath(\"./\"))\n",
    "external_libs_dir = os.path.join(cur_dir, \"signal\", \"lib\", \"external\")\n",
    "external_libs_jars = [\n",
    "    os.path.join(external_libs_dir, f) for f in os.listdir(external_libs_dir)\n",
    "]\n",
    "spark_warehouse_dir, meta_dir = \"/tmp/spark-warehouse\", \"/tmp/spark-meta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "        SparkSession.builder.master(\"local\")\n",
    "        # workaround to avoid snappy library issue\n",
    "        .config(\"spark.sql.parquet.compression.codec\", \"uncompressed\")\n",
    "        .config(\"spark.master\", \"local\")\n",
    "        .config(\"spark.driver.memory\", \"1G\")\n",
    "        # make spark-warehouse temporary\n",
    "        .config(\"spark.sql.warehouse.dir\", spark_warehouse_dir)\n",
    "        # make metastore temporary\n",
    "        .config(\n",
    "            \"spark.driver.extraJavaOptions\", \"-Dderby.system.home={}\".format(meta_dir)\n",
    "        )\n",
    "        .config(\n",
    "            \"spark.jars.repositories\",\n",
    "            \"http://packages.confluent.io/maven/,https://repo1.maven.org/maven2/\",\n",
    "        )\n",
    "        .config(\"spark.jars\", \",\".join(external_libs_jars))\n",
    "        .config(\n",
    "            \"spark.jars.packages\",\n",
    "            \"org.apache.spark:spark-avro_2.12:3.5.3,za.co.absa:abris_2.12:6.4.0,com.lihaoyi:os-lib_2.12:0.8.1,org.apache.kafka:kafka-clients:3.8.0,io.delta:delta-spark_2.12:3.2.1,org.apache.iceberg:iceberg-aws-bundle:1.6.1,org.apache.hadoop:hadoop-aws:3.2.2,org.apache.hadoop:hadoop-client:3.2.2,org.apache.hadoop:hadoop-client-runtime:3.2.2\",\n",
    "        )\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "        .appName(\"seeknal-test\")\n",
    "        .enableHiveSupport()\n",
    "        .getOrCreate()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "from seeknal.entity import Entity\n",
    "from seeknal.featurestore.feature_group import (\n",
    "    FeatureGroup,\n",
    "    Materialization,\n",
    "    OfflineMaterialization,\n",
    "    OfflineStore,\n",
    "    OfflineStoreEnum,\n",
    "    FeatureStoreFileOutput,\n",
    "    OnlineStore,\n",
    "    OnlineStoreEnum,\n",
    "    HistoricalFeatures,\n",
    "    FeatureLookup,\n",
    "    FillNull,\n",
    "    GetLatestTimeStrategy,\n",
    "    OnlineFeatures,\n",
    ")\n",
    "\n",
    "from seeknal.flow import *\n",
    "from seeknal.featurestore.featurestore import Feature\n",
    "from seeknal.common_artifact import Source, Rule, Common, Dataset\n",
    "from seeknal.project import Project\n",
    "from seeknal.workspace import Workspace\n",
    "from seeknal.tasks.sparkengine import aggregators as G\n",
    "from seeknal.tasks.sparkengine import transformers as T\n",
    "from seeknal.tasks.sparkengine.transformers.spark_engine_transformers import (\n",
    "    JoinTablesByExpr,\n",
    "    JoinType,\n",
    "    TableJoinDef,\n",
    ")\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import DataFrame\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "from src.seeknal.entity import Entity\n",
    "from src.seeknal.featurestore.feature_group import (\n",
    "    FeatureGroup,\n",
    "    Materialization,\n",
    "    OfflineMaterialization,\n",
    "    OfflineStore,\n",
    "    OfflineStoreEnum,\n",
    "    FeatureStoreFileOutput,\n",
    "    OnlineStore,\n",
    "    OnlineStoreEnum,\n",
    "    HistoricalFeatures,\n",
    "    FeatureLookup,\n",
    "    FillNull,\n",
    "    GetLatestTimeStrategy,\n",
    "    OnlineFeatures,\n",
    ")\n",
    "\n",
    "from src.seeknal.flow import *\n",
    "from src.seeknal.featurestore.featurestore import Feature\n",
    "from src.seeknal.common_artifact import Source, Rule, Common, Dataset\n",
    "from src.seeknal.project import Project\n",
    "from src.seeknal.workspace import Workspace\n",
    "from src.seeknal.tasks.sparkengine import aggregators as G\n",
    "from src.seeknal.tasks.sparkengine import transformers as T\n",
    "from src.seeknal.tasks.sparkengine.transformers.spark_engine_transformers import (\n",
    "    JoinTablesByExpr,\n",
    "    JoinType,\n",
    "    TableJoinDef,\n",
    ")\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import DataFrame\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Project and Workspace\n",
    "\n",
    "To start working with seeknal, we need to set up a project and a workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = Project(name=\"test_project\", description=\"test project\")\n",
    "# attach project\n",
    "project.get_or_create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create or use a workspace\n",
    "Workspace(name=\"test_workspace\", description=\"test workspace\").get_or_create()\n",
    "\n",
    "# check which workspace is active\n",
    "Workspace.current()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Add Common Articfacts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add source\n",
    "entity = Entity(name=\"subscriber\", join_keys=[\"msisdn\"])\n",
    "entity.get_or_create()\n",
    "source = Source(\n",
    "    \"my_user_stay\",\n",
    "    Dataset(table=\"user_stay\", params={\"dateCol\": \"date_id\"}),\n",
    "    description=\"user stay\",\n",
    "    entity=entity,\n",
    ")\n",
    "source.get_or_create()\n",
    "Source.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add rule\n",
    "Rule(name=\"foo\", value=\"bar\", description=\"foo bar\").get_or_create()\n",
    "Rule.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get common yaml\n",
    "\n",
    "Rule(name=\"foo\", value=\"bar\", description=\"foo bar\").get_or_create()\n",
    "Rule(name=\"blah\", value=[\"foo\", \"bar\"]).get_or_create()\n",
    "Source(\n",
    "    \"my_user_stay\",\n",
    "    Dataset(hive_table=\"user_stay\", params={\"dateCol\": \"date_id\"}),\n",
    "    description=\"user stay\",\n",
    ").get_or_create()\n",
    "print(Common.as_yaml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tasks\n",
    "\n",
    "DS2 build rich tasks that can be used to automate the process of data science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Spark Engine Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = \"day:string, feature1:float, feature2:float, id:string\"\n",
    "vals = [\n",
    "    (\"20190620\", 1.0, 1.0, \"1\"),\n",
    "    (\"20190610\", -1.0, -1.0, \"1\"),\n",
    "    (\"20190602\", 50.0, 50.0, \"1\"),\n",
    "    (\"20190601\", 0.0, 0.0, \"1\"),\n",
    "    (\"20190520\", 22.2, 22.2, \"1\"),\n",
    "    (\"20190510\", 2.0, 2.0, \"1\"),\n",
    "    (\"20190501\", 2.1, 2.1, \"1\"),\n",
    "    (\"20190620\", 1.0, 1.0, \"2\"),\n",
    "    (\"20190710\", None, None, \"2\"),\n",
    "    (\"20190602\", 50.0, 50.0, \"2\"),\n",
    "    (\"20190601\", 0.0, 0.0, \"2\"),\n",
    "    (\"20190520\", 22.2, 22.2, \"2\"),\n",
    "    (\"20190510\", 2.0, 2.0, \"2\"),\n",
    "    (\"20190501\", 2.1, 2.1, \"2\"),\n",
    "]\n",
    "\n",
    "daily_features_1 = spark.createDataFrame(vals, columns)\n",
    "daily_features_1.write.saveAsTable(\"test_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_str = \"\"\"\n",
    "pipeline:\n",
    "  input:\n",
    "    table: {}\n",
    "  stages:\n",
    "    - className: tech.mta.seeknal.transformers.SQL\n",
    "      params:\n",
    "        statement: >-\n",
    "          SELECT day, CONCAT(id, \"-append\") as id, feature1, feature2 FROM __THIS__\n",
    "\"\"\".format(\n",
    "    \"test_df\"\n",
    ")\n",
    "\n",
    "res = SparkEngineTask().add_yaml(yaml_str).transform(spark)\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Source(\n",
    "    \"my_test_df\",\n",
    "    Dataset(table=\"test_df\", params={\"dateCol\": \"date_id\"}),\n",
    "    description=\"just a dummy\",\n",
    ").get_or_create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform data with common yaml\n",
    "yaml_str = \"\"\"\n",
    "    pipeline:\n",
    "      input:\n",
    "        id: my_test_df\n",
    "      stages:\n",
    "        - className: tech.mta.seeknal.transformers.SQL\n",
    "          params:\n",
    "            statement: >-\n",
    "              SELECT day, CONCAT(id, \"-append\") as id, feature1, feature2 FROM __THIS__\n",
    "    \"\"\"\n",
    "\n",
    "res = (\n",
    "        SparkEngineTask()\n",
    "        .add_common_yaml(Common.as_yaml())\n",
    "        .add_yaml(yaml_str)\n",
    "        .transform(spark)\n",
    "    )\n",
    "res.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.table(\"test_df\")\n",
    "yaml_str = \"\"\"\n",
    "pipeline:\n",
    "  stages:\n",
    "    - className: tech.mta.seeknal.transformers.SQL\n",
    "      params:\n",
    "        statement: >-\n",
    "          SELECT day, CONCAT(id, \"-append\") as id, feature1, feature2 FROM __THIS__\n",
    "\"\"\"\n",
    "res = (\n",
    "    SparkEngineTask().add_yaml(yaml_str).add_input(dataframe=df).transform(spark)\n",
    ")\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_date = T.Transformer(\n",
    "    T.ClassName.ADD_DATE,\n",
    "    inputCol=\"day\",\n",
    "    outputCol=\"new_date\",\n",
    "    inputDateFormat=\"yyyyMMdd\",\n",
    "    outputDateFormat=\"yyyy-MM-dd\",\n",
    ")\n",
    "res = (\n",
    "    SparkEngineTask()\n",
    "    .add_input(dataframe=df)\n",
    "    .add_common_yaml(Common.as_yaml())\n",
    "    .add_stage(\n",
    "        transformer=T.SQL(\n",
    "            statement=\"SELECT day, CONCAT(id, '-append') as id, feature1, feature2 FROM __THIS__\"\n",
    "        )\n",
    "    )\n",
    "    .add_stage(transformer=format_date)\n",
    "    .transform(spark)\n",
    ")\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.table(\"test_df\")\n",
    "res = (\n",
    "    SparkEngineTask()\n",
    "    .add_input(dataframe=df)\n",
    "    .add_stage(\n",
    "        transformer=T.SQL(\n",
    "            statement=\"SELECT day, CONCAT(id, '-append') as id, feature1, feature2 FROM __THIS__\"\n",
    "        )\n",
    "    )\n",
    "    .transform(spark)\n",
    ")\n",
    "\n",
    "aggr = G.Aggregator(\n",
    "    group_by_cols=[\"id\"],\n",
    "    aggregators=[\n",
    "        G.FunctionAggregator(\n",
    "            inputCol=\"feature1\", outputCol=\"feature1_sum\", accumulatorFunction=\"sum\"\n",
    "        ),\n",
    "        G.ExpressionAggregator(\n",
    "            outputCol=\"feature2_sum\", expression=\"sum(feature2)\"\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "res = (\n",
    "    SparkEngineTask(name=\"create aggregates\")\n",
    "    .add_input(dataframe=df)\n",
    "    .add_stage(aggregator=aggr)\n",
    "    .add_output(\n",
    "        source=\"file\",\n",
    "        params={\"path\": \"{}/test_output\".format(\"/tmp/seeknal_result\"), \"format\": \"parquet\"},\n",
    "    )\n",
    "    .transform(spark, materialize=False)\n",
    ")\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spine_columns = \"msisdn:string, app_day:string, feature3:float\"\n",
    "spine_vals = [(\"1\", \"20190510\", 1.0), (\"2\", \"20190715\", 2.0)]\n",
    "spine_df = spark.createDataFrame(spine_vals, spine_columns)\n",
    "\n",
    "tables = [\n",
    "    TableJoinDef(\n",
    "        table=spine_df,\n",
    "        joinType=JoinType.LEFT,\n",
    "        alias=\"b\",\n",
    "        joinExpression=\"a.id = b.msisdn\",\n",
    "    )\n",
    "]\n",
    "join = JoinTablesByExpr(tables=tables, select_stm=\"a.*, b.feature3, b.app_day\")\n",
    "res = (\n",
    "    SparkEngineTask()\n",
    "    .add_input(dataframe=spark.table(\"test_df\"))\n",
    "    .add_stage(transformer=join)\n",
    "    .transform(spark)\n",
    ")\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.table(\"test_df\")\n",
    "add_month = T.Transformer(\n",
    "    T.ClassName.ADD_DATE,\n",
    "    inputCol=\"day\",\n",
    "    outputCol=\"month\",\n",
    "    inputDateFormat=\"yyyyMMdd\",\n",
    "    outputDateFormat=\"yyyy-MM-01\",\n",
    ")\n",
    "preprocess = (\n",
    "    SparkEngineTask()\n",
    "    .add_input(dataframe=df)\n",
    "    .add_stage(\n",
    "        transformer=T.SQL(\n",
    "            statement=\"SELECT day, CONCAT(id, '-append') as id, feature1, feature2 FROM __THIS__\"\n",
    "        )\n",
    "    )\n",
    ")\n",
    "res_one = preprocess.copy().add_stage(transformer=add_month).transform(spark)\n",
    "res_two = (\n",
    "    preprocess.copy()\n",
    "    .add_sql(\"SELECT *, CONCAT(id, '-append-again') as id_b FROM __THIS__\")\n",
    "    .transform(spark)\n",
    ")\n",
    "res_one.show()\n",
    "res_two.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.table(\"test_df\")\n",
    "preprocess = (\n",
    "    SparkEngineTask()\n",
    "    .add_input(dataframe=df)\n",
    "    .add_stage(\n",
    "        transformer=T.SQL(\n",
    "            statement=\"SELECT day, CONCAT(id, '-append') as id, feature1, feature2 FROM __THIS__\"\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "preprocess.update_stage(\n",
    "    0,\n",
    "    transformer=T.SQL(\n",
    "        statement=\"SELECT *, CONCAT(id, '-append-again') as id_b FROM __THIS__\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "res = preprocess.transform(spark)\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.table(\"test_df\")\n",
    "preprocess = (\n",
    "    SparkEngineTask()\n",
    "    .add_input(dataframe=df)\n",
    "    .add_stage(\n",
    "        transformer=T.SQL(\n",
    "            statement=\"SELECT day, CONCAT(id, '-append') as id, feature1, feature2 FROM __THIS__\"\n",
    "        )\n",
    "    )\n",
    ")\n",
    "preprocess.insert_stage(\n",
    "    0,\n",
    "    transformer=T.SQL(\n",
    "        statement=\"SELECT *, CONCAT(id, '-append-again') as id_b FROM __THIS__\"\n",
    "    ),\n",
    ")\n",
    "preprocess.print_yaml()\n",
    "\n",
    "res = preprocess.transform(spark)\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "df = spark.table(\"test_df\")\n",
    "\n",
    "res = (\n",
    "    SparkEngineTask()\n",
    "    .add_input(dataframe=df)\n",
    "    .set_date_col(\"day\")\n",
    "    .add_stage(\n",
    "        transformer=T.SQL(\n",
    "            statement=\"SELECT day, CONCAT(id, '-append') as id, feature1, feature2 FROM __THIS__\"\n",
    "        )\n",
    "    )\n",
    "    .transform(\n",
    "        start_date=datetime.datetime(2019, 5, 1),\n",
    "        end_date=datetime.datetime(2019, 5, 10),\n",
    "    )\n",
    ")\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "res = (\n",
    "        SparkEngineTask()\n",
    "        .add_input(dataframe=df)\n",
    "        .set_date_col(\"day\")\n",
    "        .get_date_available(after_date=datetime(2019, 5, 10))\n",
    ")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 DuckDB Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrow_df = pq.read_table(\"src/tests/data/poi_sample.parquet\")\n",
    "my_duckdb = (\n",
    "    DuckDBTask()\n",
    "    .add_input(dataframe=arrow_df)\n",
    "    .add_sql(\"SELECT poi_name, lat, long FROM __THIS__\")\n",
    "    .add_sql(\"SELECT poi_name, lat FROM __THIS__\")\n",
    ")\n",
    "print(my_duckdb.__dict__)\n",
    "res = my_duckdb.transform()\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_duckdb = (\n",
    "    DuckDBTask()\n",
    "    .add_input(path=\"seeknal/tests/data/poi_sample.parquet/*.parquet\")\n",
    "    .add_sql(\"SELECT poi_name, lat, long FROM __THIS__\")\n",
    "    .add_sql(\"SELECT poi_name, lat FROM __THIS__\")\n",
    ")\n",
    "print(my_duckdb.__dict__)\n",
    "res = my_duckdb.transform()\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seeknal.flow import (\n",
    "    Flow,\n",
    "    FlowInput,\n",
    "    FlowOutput,\n",
    "    FlowInputEnum,\n",
    "    FlowOutputEnum,\n",
    "    run_flow,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "columns = \"msisdn:string, lat:double, lon:double, start_time:string, end_time:string, count_hours:double, radius:double, movement_type:string, day:string\"\n",
    "vals = [\n",
    "        (\n",
    "            \"id1\",\n",
    "            3.1165,\n",
    "            101.5663,\n",
    "            \"2019-01-01 06:00:00\",\n",
    "            \"2019-01-01 07:00:00\",\n",
    "            1.0,\n",
    "            1395.04,\n",
    "            \"stay\",\n",
    "            \"20190101\",\n",
    "        ),\n",
    "        (\n",
    "            \"id2\",\n",
    "            3.812033,\n",
    "            103.324633,\n",
    "            \"2019-01-01 06:00:00\",\n",
    "            \"2019-01-01 07:00:00\",\n",
    "            1.0,\n",
    "            841.36,\n",
    "            \"stay\",\n",
    "            \"20190101\",\n",
    "        ),\n",
    "        (\n",
    "            \"id3\",\n",
    "            3.0637,\n",
    "            101.47016,\n",
    "            \"2019-01-01 06:00:00\",\n",
    "            \"2019-01-01 07:00:00\",\n",
    "            1.0,\n",
    "            1387.35,\n",
    "            \"stay\",\n",
    "            \"20190101\",\n",
    "        ),\n",
    "        (\n",
    "            \"id1\",\n",
    "            3.1186,\n",
    "            101.6639,\n",
    "            \"2019-01-01 07:00:00\",\n",
    "            \"2019-01-01 08:00:00\",\n",
    "            1.0,\n",
    "            1234.22,\n",
    "            \"stay\",\n",
    "            \"20190101\",\n",
    "        ),\n",
    "        (\n",
    "            \"id1\",\n",
    "            3.1165,\n",
    "            101.5663,\n",
    "            \"2019-01-01 06:00:00\",\n",
    "            \"2019-01-01 07:00:00\",\n",
    "            1.0,\n",
    "            1395.04,\n",
    "            \"stay\",\n",
    "            \"20190102\",\n",
    "        ),\n",
    "        (\n",
    "            \"id2\",\n",
    "            3.812033,\n",
    "            103.324633,\n",
    "            \"2019-01-01 06:00:00\",\n",
    "            \"2019-01-01 07:00:00\",\n",
    "            1.0,\n",
    "            841.36,\n",
    "            \"stay\",\n",
    "            \"20190102\",\n",
    "        ),\n",
    "        (\n",
    "            \"id3\",\n",
    "            3.0637,\n",
    "            101.47016,\n",
    "            \"2019-01-01 06:00:00\",\n",
    "            \"2019-01-01 07:00:00\",\n",
    "            1.0,\n",
    "            1387.35,\n",
    "            \"stay\",\n",
    "            \"20190102\",\n",
    "        ),\n",
    "        (\n",
    "            \"id1\",\n",
    "            3.1186,\n",
    "            101.6639,\n",
    "            \"2019-01-01 07:00:00\",\n",
    "            \"2019-01-01 08:00:00\",\n",
    "            1.0,\n",
    "            1234.22,\n",
    "            \"stay\",\n",
    "            \"20190102\",\n",
    "        ),\n",
    "        (\n",
    "            \"id1\",\n",
    "            3.1165,\n",
    "            101.5663,\n",
    "            \"2019-01-01 06:00:00\",\n",
    "            \"2019-01-01 07:00:00\",\n",
    "            1.0,\n",
    "            1395.04,\n",
    "            \"stay\",\n",
    "            \"20190105\",\n",
    "        ),\n",
    "        (\n",
    "            \"id2\",\n",
    "            3.812033,\n",
    "            103.324633,\n",
    "            \"2019-01-01 06:00:00\",\n",
    "            \"2019-01-01 07:00:00\",\n",
    "            1.0,\n",
    "            841.36,\n",
    "            \"stay\",\n",
    "            \"20190105\",\n",
    "        ),\n",
    "        (\n",
    "            \"id3\",\n",
    "            3.0637,\n",
    "            101.47016,\n",
    "            \"2019-01-01 06:00:00\",\n",
    "            \"2019-01-01 07:00:00\",\n",
    "            1.0,\n",
    "            1387.35,\n",
    "            \"stay\",\n",
    "            \"20190105\",\n",
    "        ),\n",
    "        (\n",
    "            \"id1\",\n",
    "            3.1186,\n",
    "            101.6639,\n",
    "            \"2019-01-01 07:00:00\",\n",
    "            \"2019-01-01 08:00:00\",\n",
    "            1.0,\n",
    "            1234.22,\n",
    "            \"stay\",\n",
    "            \"20190105\",\n",
    "        ),\n",
    "    ]\n",
    "user_stay = spark.createDataFrame(vals, columns)\n",
    "(\n",
    "    SparkEngineTask()\n",
    "    .add_input(dataframe=user_stay)\n",
    "    .transform()\n",
    "    .write.mode(\"overwrite\")\n",
    "    .saveAsTable(\"user_stay\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = Project(name=\"test_project\", description=\"test project\")\n",
    "# attach project\n",
    "project.get_or_create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_input = FlowInput(kind=FlowInputEnum.HIVE_TABLE, value=\"user_stay\")\n",
    "flow_output = FlowOutput(kind=FlowOutputEnum.SPARK_DATAFRAME)\n",
    "\n",
    "task_two = SparkEngineTask().add_sql(\"SELECT * FROM __THIS__\")\n",
    "task_three = DuckDBTask().add_sql(\"SELECT msisdn, lat, lon, movement_type, day FROM __THIS__\")\n",
    "flow = Flow(\n",
    "    name=\"my_flow\",\n",
    "    input=flow_input,\n",
    "    tasks=[task_two, task_three],\n",
    "    output=FlowOutput(),\n",
    ")\n",
    "\n",
    "flow.get_or_create()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save from the object\n",
    "print(flow.run())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from seeknal\n",
    "flowtwo = Flow(name=\"my_flow\").get_or_create()\n",
    "print(flowtwo.run())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow.as_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_flow = Flow.from_dict(flow.as_dict())\n",
    "my_flow.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Feature Group "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comm_day = spark.read.format(\"parquet\").load(\"seeknal/tests/data/feateng_comm_day\")\n",
    "comm_day.write.saveAsTable(\"comm_day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.table(\"comm_day\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"DROP TABLE seeknal.fg_1__1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FeatureStoreFileOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "materialization = Materialization(event_time_col=\"day\", \n",
    "offline_materialization=OfflineMaterialization(\n",
    "    store=OfflineStore(kind=OfflineStoreEnum.FILE, \n",
    "                       name=\"object_storage\",\n",
    "                       value=FeatureStoreFileOutput(path=\"s3a://warehouse/feature_store\")), \n",
    "                       mode=\"overwrite\", ttl=None),\n",
    "    offline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_input = FlowInput(kind=FlowInputEnum.HIVE_TABLE, value=\"comm_day\")\n",
    "my_flow = Flow(\n",
    "    input=flow_input, tasks=None, output=FlowOutput(), name=\"my_flow_for_fg\"\n",
    ")\n",
    "\n",
    "my_fg_one = FeatureGroup(\n",
    "    name=\"comm_day_three\",\n",
    "    entity=Entity(name=\"msisdn\", join_keys=[\"msisdn\"]).get_or_create(),\n",
    "    materialization=materialization,\n",
    ").set_flow(my_flow)\n",
    "my_fg_one.set_features()\n",
    "# print(my_fg)\n",
    "my_fg_one.get_or_create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "my_fg_one.write(\n",
    "        feature_start_time=datetime(2019, 3, 5)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_input = FlowInput(kind=FlowInputEnum.HIVE_TABLE, value=\"comm_day\")\n",
    "my_flow = Flow(\n",
    "    input=flow_input, tasks=None, output=FlowOutput(), name=\"my_flow_for_fg\"\n",
    ")\n",
    "\n",
    "my_fg_one = FeatureGroup(\n",
    "    name=\"comm_day\",\n",
    "    entity=Entity(name=\"msisdn\", join_keys=[\"msisdn\"]).get_or_create(),\n",
    "    materialization=Materialization(event_time_col=\"day\"),\n",
    ").set_flow(my_flow)\n",
    "my_fg_one.set_features()\n",
    "# print(my_fg)\n",
    "my_fg_one.get_or_create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_fg_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = spark.read.table(\"comm_day\")\n",
    "my_fg_two = FeatureGroup(\n",
    "    name=\"comm_day_two\",\n",
    "    entity=Entity(name=\"msisdn\", join_keys=[\"msisdn\"]).get_or_create(),\n",
    "    materialization=Materialization(event_time_col=\"day\"),\n",
    ").set_dataframe(dataframe=input_df)\n",
    "\n",
    "my_fg_two.set_features()\n",
    "my_fg_two.get_or_create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "my_fg_one.write(\n",
    "        feature_start_time=datetime(2019, 3, 5)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_fg = FeatureGroup(name=\"comm_day_two\").get_or_create()\n",
    "my_fg.set_dataframe(dataframe=spark.read.table(\"comm_day\")).write(\n",
    "    feature_start_time=datetime(2019, 3, 5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Historical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_fg = FeatureGroup(name=\"comm_day_two\")\n",
    "fs = FeatureLookup(source=my_fg)\n",
    "fillnull = FillNull(value=\"0.0\", dataType=\"double\")\n",
    "hist = HistoricalFeatures(lookups=[fs], fill_nulls=[fillnull])\n",
    "df = hist.to_dataframe(feature_start_time=datetime(2019, 3, 5))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_fg = FeatureGroup(name=\"comm_day_three\")\n",
    "my_fg_two = FeatureGroup(name=\"comm_day_three\")\n",
    "fs = FeatureLookup(source=my_fg)\n",
    "fs_two = FeatureLookup(source=my_fg_two, features=[\"comm_count_call_in\"])\n",
    "fillnull = FillNull(value=\"0.0\", dataType=\"double\")\n",
    "hist = HistoricalFeatures(lookups=[fs], fill_nulls=[fillnull])\n",
    "df = hist.to_dataframe(feature_start_time=datetime(2019, 3, 5))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_fg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_fg = FeatureGroup(name=\"comm_day\")\n",
    "fs = FeatureLookup(source=my_fg)\n",
    "\n",
    "hist = HistoricalFeatures(lookups=[fs])\n",
    "spine_dummy_data = pd.DataFrame(\n",
    "    [\n",
    "        {\"msisdn\": \"011ezY2Kjs\", \"app_date\": \"2019-03-19\", \"label\": 1},\n",
    "        {\"msisdn\": \"01ViZtJZCj\", \"app_date\": \"2019-03-10\", \"label\": 0},\n",
    "    ]\n",
    ")\n",
    "df = hist.using_spine(\n",
    "    spine=spine_dummy_data, date_col=\"app_date\", keep_cols=[\"label\"]\n",
    ").to_dataframe()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_fg = FeatureGroup(name=\"comm_day\")\n",
    "my_fg_two = FeatureGroup(name=\"comm_day_two\")\n",
    "fs = FeatureLookup(source=my_fg)\n",
    "fs_two = FeatureLookup(source=my_fg_two)\n",
    "\n",
    "hist = HistoricalFeatures(lookups=[fs, fs_two])\n",
    "df = hist.using_latest(\n",
    "    fetch_strategy=GetLatestTimeStrategy.REQUIRE_ANY\n",
    ").to_dataframe()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Serve features to Online Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_fg = FeatureGroup(name=\"comm_day_three\")\n",
    "fs = FeatureLookup(source=my_fg)\n",
    "\n",
    "user_one = Entity(name=\"msisdn\").get_or_create().set_key_values(\"05X5wBWKN3\")\n",
    "fillnull = FillNull(value=\"0.0\", dataType=\"double\")\n",
    "hist = HistoricalFeatures(lookups=[fs], fill_nulls=[fillnull])\n",
    "hist = hist.using_latest().serve()\n",
    "abc = hist.get_features(keys=[user_one])\n",
    "print(abc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_fg = FeatureGroup(name=\"comm_day_two\")\n",
    "fs = FeatureLookup(source=my_fg)\n",
    "\n",
    "user_one = Entity(name=\"msisdn\").get_or_create().set_key_values(\"05X5wBWKN3\")\n",
    "fillnull = FillNull(value=\"0.0\", dataType=\"double\")\n",
    "hist = HistoricalFeatures(lookups=[fs], fill_nulls=[fillnull])\n",
    "online_store = OnlineStore(value=FeatureStoreFileOutput(path=\"/tmp/online_store\"))\n",
    "online_table = hist.using_latest().serve(\n",
    "    target=online_store, ttl=timedelta(minutes=1)\n",
    ")\n",
    "print(online_table.get_features(keys=[user_one]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = spark.read.table(\"comm_day\").withColumnRenamed(\"day\", \"event_time\")\n",
    "online_table = OnlineFeatures(\n",
    "    lookup_key=Entity(name=\"msisdn\").get_or_create(), dataframe=abc\n",
    ")\n",
    "abc = online_table.get_features(keys=[{\"msisdn\": \"05X5wBWKN3\"}])\n",
    "print(abc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
