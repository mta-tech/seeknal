name: python-postgresql-medallion
description: >
  Test Python pipeline decorators with PostgreSQL sources and multi-target materialization.
  Validates: @source with postgres connection profiles, @transform with ctx.ref(),
  stackable @materialize decorators for multi-target (PostgreSQL + Iceberg),
  all PostgreSQL write modes (full, upsert_by_key), source_defaults with alias
  normalization (postgres -> postgresql), PythonExecutor subprocess + post_execute bridge.
source_type: postgresql
pipeline_type: python

infrastructure:
  requires: [postgresql, lakekeeper]
  health_checks:
    postgresql: "pg_isready -h localhost -p 5432 -U seeknal"
    lakekeeper: "curl -s --connect-timeout 5 -o /dev/null -w '%{http_code}' http://172.19.0.9:8181/catalog/v1/config?warehouse=seeknal-warehouse | grep -qE '^(200|401)'"

env:
  PG_HOST: "localhost"
  PG_PORT: "5432"
  PG_USER: "seeknal"
  PG_PASSWORD: "seeknal_pass"
  PG_DATABASE: "seeknal_test"
  LAKEKEEPER_URI: "http://172.19.0.9:8181"
  LAKEKEEPER_WAREHOUSE_ID: "c008ea5c-fb89-11f0-aa64-c32ca2f52144"
  LAKEKEEPER_WAREHOUSE: "seeknal-warehouse"
  KEYCLOAK_TOKEN_URL: "http://172.19.0.9:8080/realms/atlas/protocol/openid-connect/token"
  KEYCLOAK_CLIENT_ID: "duckdb"
  KEYCLOAK_CLIENT_SECRET: "duckdb-secret-change-in-production"
  AWS_ACCESS_KEY_ID: "minioadmin"
  AWS_SECRET_ACCESS_KEY: "CHANGE_THIS_STRONG_PASSWORD"
  AWS_ENDPOINT_URL: "http://172.19.0.9:9000"
  AWS_REGION: "us-east-1"

profiles_yml: |
  connections:
    local_pg:
      type: postgresql
      host: "${PG_HOST:localhost}"
      port: ${PG_PORT:5432}
      database: "${PG_DATABASE:seeknal_test}"
      user: "${PG_USER:seeknal}"
      password: "${PG_PASSWORD:seeknal_pass}"

  source_defaults:
    postgres:
      connection: local_pg
    iceberg:
      catalog_uri: "${LAKEKEEPER_URI:http://172.19.0.9:8181}"
      warehouse: "${LAKEKEEPER_WAREHOUSE:seeknal-warehouse}"

  materialization:
    enabled: true
    catalog:
      type: rest
      uri: "${LAKEKEEPER_URI:http://172.19.0.9:8181}"
      warehouse: "${LAKEKEEPER_WAREHOUSE:seeknal-warehouse}"

seed_sql: |
  -- Seed data for Python PostgreSQL medallion QA test
  CREATE SCHEMA IF NOT EXISTS bronze;
  CREATE SCHEMA IF NOT EXISTS analytics;

  DROP TABLE IF EXISTS bronze.sales_orders CASCADE;
  CREATE TABLE bronze.sales_orders (
    order_id VARCHAR(20) PRIMARY KEY,
    customer_id VARCHAR(10) NOT NULL,
    product_id VARCHAR(10) NOT NULL,
    quantity INTEGER NOT NULL,
    unit_price DECIMAL(10,2) NOT NULL,
    order_date DATE NOT NULL,
    region VARCHAR(20) NOT NULL
  );

  INSERT INTO bronze.sales_orders VALUES
    ('SO001', 'C001', 'P001', 2, 29.99, '2026-01-15', 'north'),
    ('SO002', 'C002', 'P002', 1, 49.99, '2026-01-15', 'south'),
    ('SO003', 'C001', 'P003', 3, 19.99, '2026-01-16', 'north'),
    ('SO004', 'C003', 'P001', 1, 29.99, '2026-01-16', 'east'),
    ('SO005', 'C002', 'P004', 2, 39.99, '2026-01-17', 'south'),
    ('SO006', 'C004', 'P002', 1, 49.99, '2026-01-17', 'west'),
    ('SO007', 'C001', 'P001', 4, 29.99, '2026-01-18', 'north'),
    ('SO008', 'C003', 'P003', 2, 19.99, '2026-01-18', 'east'),
    ('SO009', 'C005', 'P004', 1, 39.99, '2026-01-19', 'north'),
    ('SO010', 'C004', 'P001', 3, 29.99, '2026-01-19', 'west'),
    ('SO011', 'C002', 'P002', 2, 49.99, '2026-01-20', 'south'),
    ('SO012', 'C001', 'P003', 1, 19.99, '2026-01-20', 'north');

  DROP TABLE IF EXISTS bronze.products CASCADE;
  CREATE TABLE bronze.products (
    product_id VARCHAR(10) PRIMARY KEY,
    product_name VARCHAR(100) NOT NULL,
    category VARCHAR(50) NOT NULL,
    cost_price DECIMAL(10,2) NOT NULL
  );

  INSERT INTO bronze.products VALUES
    ('P001', 'Widget Alpha', 'widgets', 15.00),
    ('P002', 'Gadget Beta', 'gadgets', 25.00),
    ('P003', 'Widget Gamma', 'widgets', 10.00),
    ('P004', 'Gadget Delta', 'gadgets', 20.00);

  DROP TABLE IF EXISTS bronze.regions CASCADE;
  CREATE TABLE bronze.regions (
    region VARCHAR(20) PRIMARY KEY,
    region_name VARCHAR(50) NOT NULL,
    timezone VARCHAR(30) NOT NULL
  );

  INSERT INTO bronze.regions VALUES
    ('north', 'Northern Region', 'Asia/Jakarta'),
    ('south', 'Southern Region', 'Asia/Makassar'),
    ('east', 'Eastern Region', 'Asia/Jayapura'),
    ('west', 'Western Region', 'Asia/Jakarta');

seed_data: {}

pipeline_files:
  medallion.py: |
    """Python PostgreSQL medallion pipeline for QA testing.

    Tests @source with PostgreSQL connection profiles, @transform with ctx.ref(),
    stackable @materialize for multi-target (PostgreSQL + Iceberg), and all
    PostgreSQL write modes.
    """
    from seeknal.pipeline.decorators import source, transform, materialize

    # --- Bronze layer: PostgreSQL sources ---
    # connection: local_pg supplied by source_defaults (postgres -> postgresql alias)

    @source(name="sales_orders", source="postgres",
            table="bronze.sales_orders",
            tags=["bronze", "orders"])
    def sales_orders():
        pass

    @source(name="recent_orders", source="postgres",
            table="bronze.sales_orders",
            query="SELECT * FROM bronze.sales_orders WHERE order_date >= '2026-01-18'",
            tags=["bronze", "orders", "pushdown"])
    def recent_orders():
        pass

    @source(name="products", source="postgres",
            table="bronze.products",
            tags=["bronze", "products"])
    def products():
        pass

    @source(name="regions", source="postgres",
            table="bronze.regions",
            tags=["bronze", "reference"])
    def regions():
        pass

    # --- Silver layer: enriched with multi-target materialization ---

    @transform(
        name="order_details",
        inputs=["source.sales_orders", "source.products", "source.regions"],
        tags=["silver", "enriched"],
    )
    @materialize(type="postgresql", connection="local_pg",
                 table="analytics.order_details", mode="full")
    @materialize(type="iceberg",
                 table="atlas.qa_py_pg.silver_order_details")
    def order_details(ctx):
        orders = ctx.ref("source.sales_orders")
        prods = ctx.ref("source.products")
        regs = ctx.ref("source.regions")
        return ctx.duckdb.sql("""
            SELECT
                o.order_id,
                o.customer_id,
                o.product_id,
                p.product_name,
                p.category,
                o.quantity,
                o.unit_price,
                CAST(o.quantity * o.unit_price AS DECIMAL(10,2)) AS line_total,
                CAST(o.quantity * p.cost_price AS DECIMAL(10,2)) AS line_cost,
                CAST(o.quantity * (o.unit_price - p.cost_price) AS DECIMAL(10,2)) AS line_margin,
                o.order_date,
                o.region,
                r.region_name,
                r.timezone
            FROM orders o
            JOIN prods p ON o.product_id = p.product_id
            JOIN regs r ON o.region = r.region
        """).df()

    # --- Gold layer: aggregations with different PostgreSQL modes ---

    @transform(
        name="customer_metrics",
        inputs=["transform.order_details"],
        tags=["gold", "analytics"],
    )
    @materialize(type="postgresql", connection="local_pg",
                 table="analytics.customer_metrics",
                 mode="upsert_by_key", unique_keys=["customer_id"])
    @materialize(type="iceberg",
                 table="atlas.qa_py_pg.gold_customer_metrics")
    def customer_metrics(ctx):
        details = ctx.ref("transform.order_details")
        return ctx.duckdb.sql("""
            SELECT
                customer_id,
                COUNT(DISTINCT order_id) AS total_orders,
                COUNT(DISTINCT product_id) AS unique_products,
                CAST(SUM(line_total) AS DECIMAL(10,2)) AS total_spent,
                CAST(AVG(line_total) AS DECIMAL(10,2)) AS avg_order_value,
                MIN(order_date) AS first_order,
                MAX(order_date) AS last_order
            FROM details
            GROUP BY customer_id
        """).df()

    @transform(
        name="category_performance",
        inputs=["transform.order_details"],
        tags=["gold", "analytics"],
    )
    @materialize(type="postgresql", connection="local_pg",
                 table="analytics.py_category_performance", mode="full")
    def category_performance(ctx):
        details = ctx.ref("transform.order_details")
        return ctx.duckdb.sql("""
            SELECT
                category,
                COUNT(*) AS total_line_items,
                CAST(SUM(line_total) AS DECIMAL(10,2)) AS total_revenue,
                CAST(SUM(line_margin) AS DECIMAL(10,2)) AS total_margin,
                CAST(
                    SUM(line_margin) / NULLIF(SUM(line_total), 0) * 100
                    AS DECIMAL(5,2)
                ) AS margin_pct
            FROM details
            GROUP BY category
        """).df()

pipeline:
  bronze:
    - kind: source
      name: sales_orders
      source: postgres
    - kind: source
      name: recent_orders
      source: postgres
    - kind: source
      name: products
      source: postgres
    - kind: source
      name: regions
      source: postgres
  silver:
    - kind: transform
      name: order_details
      inputs:
        - ref: source.sales_orders
        - ref: source.products
        - ref: source.regions
  gold:
    - kind: transform
      name: customer_metrics
      inputs:
        - ref: transform.order_details
    - kind: transform
      name: category_performance
      inputs:
        - ref: transform.order_details

validation:
  dag:
    expected_nodes: 7
    expected_edges:
      - [source.sales_orders, transform.order_details]
      - [source.products, transform.order_details]
      - [source.regions, transform.order_details]
      - [transform.order_details, transform.customer_metrics]
      - [transform.order_details, transform.category_performance]
  execution:
    success: true
  outputs:
    - node: transform.order_details
      min_rows: 12
    - node: transform.customer_metrics
      min_rows: 5
    - node: transform.category_performance
      min_rows: 2
  postgresql_tables:
    - schema: analytics
      table: order_details
      min_rows: 12
    - schema: analytics
      table: customer_metrics
      min_rows: 5
    - schema: analytics
      table: py_category_performance
      min_rows: 2
  source_defaults_check:
    - node: source.sales_orders
      param: connection
      expected: "local_pg"
  pushdown_check:
    - node: source.recent_orders
      max_rows: 8

features_tested:
  - python_source_decorator
  - python_transform_decorator
  - ctx_ref_input_loading
  - ctx_duckdb_sql
  - duckdb_variable_shadowing_pattern
  - python_executor_subprocess
  - materialize_decorator_stackable
  - multi_target_materialization_python
  - postgresql_source_python
  - postgresql_connection_profile_python
  - postgresql_mode_full_python
  - postgresql_mode_upsert_by_key_python
  - pushdown_query_python
  - source_defaults_python
  - source_defaults_alias_normalization_python
  - iceberg_materialization_python
  - post_execute_parquet_bridge
  - multi_source_join_python
