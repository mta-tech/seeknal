---
title: "Iceberg Materialization for Feature Groups"
type: feat
date: 2026-02-10
status: ready
bmad_source: null
---

# Plan: Iceberg Materialization for Feature Groups

## Overview

Add Iceberg table materialization support to Seeknal Feature Groups, enabling features to be stored in Iceberg tables with ACID transactions, time travel, and cloud storage compatibility. Implementation reuses existing YAML pipeline Iceberg infrastructure and maintains full backward compatibility with existing `FeatureGroup.write()` API.

**Key Deliverables:**
- New `OfflineStoreEnum.ICEBERG` storage option for Feature Groups
- `IcebergStoreOutput` configuration model for catalog/warehouse settings
- Iceberg write path in `OfflineStore.__call__()` using existing infrastructure
- Real integration testing with MinIO + Lakekeeper on atlas-dev-server
- Full backward compatibility with existing FILE and HIVE_TABLE backends

**Architecture Note:**
This implementation reuses the existing Iceberg infrastructure from YAML pipelines (`src/seeknal/workflow/materialization/`), including:
- `CatalogConfig` for catalog configuration
- `ProfileLoader` for loading profiles.yml credentials
- `DuckDBIcebergExtension` for DuckDB Iceberg operations
- `write_to_iceberg()` for atomic table writes

## Task Description

Add Apache Iceberg as a storage backend for Seeknal Feature Groups, allowing ML engineers to store features in Iceberg tables instead of local Parquet files. Features stored in Iceberg gain ACID transaction guarantees, time travel capabilities, schema evolution support, and cloud storage compatibility (S3, GCS, Azure Blob).

**Target Users:**
- ML engineers who need shareable feature stores
- Data teams requiring ACID guarantees for feature writes
- Organizations with existing Iceberg/Lakekeeper infrastructure

**Value Proposition:**
- Store features in cloud-native Iceberg tables instead of local filesystem
- Query features from multiple systems (DuckDB, Spark, Trino)
- Time travel: query features as of any historical point
- Schema evolution: add/modify features without data rewrites

## Objective

Enable Feature Groups to use Apache Iceberg as a storage backend while maintaining full backward compatibility with existing APIs and storage options (FILE, HIVE_TABLE).

**Success Criteria:**
- Users can create Feature Groups with `OfflineStoreEnum.ICEBERG`
- Features materialize to Iceberg tables on MinIO via Lakekeeper catalog
- Existing Feature Group code continues to work without changes
- Integration tests pass against real MinIO + Lakekeeper infrastructure
- Performance comparable to existing Parquet storage

## Problem Statement

### Current Limitations

Seeknal Feature Groups currently store materialized features in:
- **File backend**: `~/.seeknal/data/` or `target/` directories (Parquet format)
- **Hive Table backend**: Hive metastore tables

**User Pain Points:**
- Feature data stored locally, not accessible to other systems
- No time travel capabilities for feature versions
- No ACID guarantees during concurrent writes
- Difficult to share features across teams/environments
- Storage tied to local filesystem, not cloud-native
- Cannot query features from Spark, Trino, or other compute engines

### Why Iceberg?

Apache Iceberg provides:
- **ACID transactions**: Atomic writes with automatic rollback
- **Time travel**: Query features as of any point in time
- **Schema evolution**: Add/modify features without rewrites
- **Cloud storage**: S3, GCS, Azure Blob support
- **Compatibility**: Works with DuckDB, Spark, Trino, and more

## Proposed Solution

Add `ICEBERG` as a new `OfflineStoreKind` option for Feature Groups, enabling Iceberg storage through the existing `FeatureGroup.write()` API.

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FeatureGroup.write()                      â”‚
â”‚                                                             â”‚
â”‚  fg = FeatureGroup(                                         â”‚
â”‚      name="user_features",                                  â”‚
â”‚      entity=customer_entity,                                â”‚
â”‚      materialization=Materialization(                        â”‚
â”‚          offline=True,                                       â”‚
â”‚          offline_materialization=OfflineMaterialization(     â”‚
â”‚              store=OfflineStore(                             â”‚
â”‚                  kind=OfflineStoreEnum.ICEBERG,  # NEW!     â”‚
â”‚                  value=IcebergStoreOutput(                   â”‚
â”‚                      catalog="lakekeeper",                   â”‚
â”‚                      warehouse="s3://warehouse",             â”‚
â”‚                      namespace="prod",                       â”‚
â”‚                      table="user_features"                  â”‚
â”‚                  )                                           â”‚
â”‚              ),                                               â”‚
â”‚              mode="append"                                   â”‚
â”‚          )                                                   â”‚
â”‚      )                                                       â”‚
â”‚  )                                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               OfflineStore.__call__()                        â”‚
â”‚                                                             â”‚
â”‚  if kind == OfflineStoreEnum.ICEBERG:                       â”‚
â”‚      - Load DuckDB Iceberg extension                         â”‚
â”‚      - Setup Lakekeeper REST catalog                        â”‚
â”‚      - Write features to Iceberg table                      â”‚
â”‚      - Return snapshot ID                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   atlas-dev-server Infrastructure            â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  MinIO      â”‚â”€â”€â”€â”€â–¶â”‚ Lakekeeper  â”‚â”€â”€â”€â”€â–¶â”‚ PostgreSQL  â”‚  â”‚
â”‚  â”‚  :9000      â”‚     â”‚  :8181      â”‚     â”‚  :5432      â”‚  â”‚
â”‚  â”‚  S3 Storage â”‚     â”‚ REST Catalogâ”‚     â”‚  Metadata   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â”‚                                        â”‚         â”‚
â”‚         â–¼                                        â–¼         â”‚
â”‚   Iceberg Tables                            Warehouse       â”‚
â”‚   (s3://warehouse/...)                      Configuration   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Implementation Approach

Reuse existing Iceberg infrastructure from YAML pipelines:

- **Config models**: Reuse `CatalogConfig` from `workflow/materialization/config.py`
- **Operations**: Reuse `write_to_iceberg()` from `workflow/materialization/operations.py`
- **Profile loader**: Reuse `ProfileLoader` from `workflow/materialization/profile_loader.py`
- **DuckDB extension**: Reuse `DuckDBIcebergExtension` from `workflow/materialization/operations.py`

This ensures consistency and reduces duplication.

## Relevant Files

### Existing Files to Modify

| File | Purpose | Changes |
|------|---------|---------|
| `src/seeknal/featurestore/featurestore.py` | Add ICEBERG enum and IcebergStoreOutput model | Add new enum value and dataclass |
| `src/seeknal/featurestore/featurestore.py` | OfflineStore.__call__() | Add _write_to_iceberg() method |
| `src/seeknal/featurestore/feature_group.py` | FeatureGroup.write() | Capture Iceberg snapshot metadata |
| `src/seeknal/featurestore/__init__.py` | Public API exports | Export new types |

### New Files to Create

| File | Purpose |
|------|---------|
| `tests/featurestore/test_iceberg_materialization.py` | Unit tests for Iceberg write path |
| `tests/integration/test_iceberg_real.py` | Integration tests with real MinIO/Lakekeeper |
| `examples/iceberg_feature_group.py` | Example notebook showing usage |

### Reference Files

| File | Purpose |
|------|---------|
| `src/seeknal/workflow/materialization/operations.py` | DuckDB Iceberg operations to reuse |
| `src/seeknal/workflow/materialization/config.py` | CatalogConfig to reuse |
| `src/seeknal/workflow/materialization/profile_loader.py` | ProfileLoader to reuse |
| `src/seeknal/workflow/materialization/yaml_integration.py` | Reference implementation |

## Step by Step Tasks

### 1. Add ICEBERG to OfflineStoreEnum

**File**: `src/seeknal/featurestore/featurestore.py`

**Task**: Add ICEBERG as new enum value

```python
class OfflineStoreEnum(str, Enum):
    FILE = "file"
    HIVE_TABLE = "hive_table"
    ICEBERG = "iceberg"  # NEW
```

**Acceptance Criteria:**
- [ ] `OfflineStoreEnum.ICEBERG` exists with value "iceberg"
- [ ] No changes to existing enum values (backward compatible)

**Dependencies:** None

**Assigned To:** feature-dev-backend-agent

**Agent Type:** backend-agent

**Parallel:** false

---

### 2. Create IcebergStoreOutput Model

**File**: `src/seeknal/featurestore/featurestore.py`

**Task**: Create dataclass for Iceberg storage configuration

```python
@dataclass
class IcebergStoreOutput:
    """Iceberg storage configuration for Feature Groups.

    Attributes:
        catalog: Catalog name from profiles.yml (default: "lakekeeper")
        warehouse: Optional warehouse path override (default: from profile)
        namespace: Iceberg namespace/database name (default: "default")
        table: Table name within namespace (required)
        mode: Write mode - "append" or "overwrite" (default: "append")
    """
    catalog: str = "lakekeeper"
    warehouse: Optional[str] = None
    namespace: str = "default"
    table: str = ""
    mode: str = "append"
```

**Acceptance Criteria:**
- [ ] `IcebergStoreOutput` dataclass created with all fields
- [ ] Default values match YAML pipeline conventions
- [ ] Type hints use Optional for warehouse override
- [ ] Docstring explains each field purpose

**Dependencies:** Task 1 (OfflineStoreEnum.ICEBERG must exist)

**Assigned To:** feature-dev-backend-agent

**Agent Type:** backend-agent

**Parallel:** false

---

### 3. Update OfflineStore Type Hints

**File**: `src/seeknal/featurestore/featurestore.py`

**Task**: Update OfflineStore to accept IcebergStoreOutput

```python
from typing import Union

class OfflineStore:
    kind: OfflineStoreEnum = OfflineStoreEnum.HIVE_TABLE
    value: Union[FeatureStoreFileOutput, FeatureStoreHiveTableOutput, IcebergStoreOutput]
    name: str
```

**Acceptance Criteria:**
- [ ] `value` type hint includes `IcebergStoreOutput`
- [ ] No breaking changes to existing type signatures
- [ ] Union properly handles all three store types

**Dependencies:** Task 2 (IcebergStoreOutput must exist)

**Assigned To:** feature-dev-backend-agent

**Agent Type:** backend-agent

**Parallel:** false

---

### 4. Implement _write_to_iceberg() Method

**File**: `src/seeknal/featurestore/featurestore.py`

**Task**: Implement Iceberg write logic in OfflineStore class

```python
def _write_to_iceberg(self, df, project_name):
    """Write DataFrame to Iceberg table via Lakekeeper catalog.

    Args:
        df: Spark DataFrame or Pandas DataFrame
        project_name: Project name for namespacing

    Returns:
        Dict with path, row_count, storage_type, and metadata

    Raises:
        ValueError: If Iceberg configuration is invalid
        RuntimeError: If catalog connection or write fails
    """
    from seeknal.workflow.materialization.operations import DuckDBIcebergExtension
    from seeknal.workflow.materialization.profile_loader import ProfileLoader
    import duckdb

    # Validate configuration
    iceberg_config = self.value
    if not iceberg_config.table:
        raise ValueError("IcebergStoreOutput.table is required")

    # Convert Spark DataFrame to Pandas if needed
    if hasattr(df, 'toPandas'):
        df = df.toPandas()

    # Load profile for catalog configuration
    profile_loader = ProfileLoader()
    profile_config = profile_loader.load_profile()

    # Use config values or fall back to profile
    catalog_uri = iceberg_config.catalog or profile_config.catalog.uri
    warehouse_path = iceberg_config.warehouse or profile_config.catalog.warehouse

    # Create DuckDB connection
    con = duckdb.connect(":memory:")

    try:
        # Load Iceberg extension
        DuckDBIcebergExtension.load_extension(con)

        # Setup REST catalog
        catalog_name = "seeknal_catalog"
        bearer_token = profile_config.catalog.bearer_token

        DuckDBIcebergExtension.create_rest_catalog(
            con=con,
            catalog_name=catalog_name,
            uri=catalog_uri,
            warehouse_path=warehouse_path,
            bearer_token=bearer_token,
        )

        # Create table reference
        table_ref = f"{catalog_name}.{iceberg_config.namespace}.{iceberg_config.table}"

        # Register DataFrame as view
        con.register('features_df', df)

        # Write to Iceberg based on mode
        if iceberg_config.mode == "overwrite":
            con.execute(f"DROP TABLE IF EXISTS {table_ref}")
            con.execute(f"CREATE TABLE {table_ref} AS SELECT * FROM features_df")
        else:
            # Append mode
            try:
                con.execute(f"INSERT INTO {table_ref} SELECT * FROM features_df")
            except:
                # Table doesn't exist, create it
                con.execute(f"CREATE TABLE {table_ref} AS SELECT * FROM features_df")

        # Get row count and verify
        row_count = con.execute(f"SELECT COUNT(*) FROM {table_ref}").fetchone()[0]

        return {
            "path": table_ref,
            "num_rows": row_count,
            "storage_type": "iceberg",
            "catalog": iceberg_config.catalog,
            "namespace": iceberg_config.namespace,
            "table": iceberg_config.table,
        }

    finally:
        con.close()
```

**Acceptance Criteria:**
- [ ] Method handles both Spark and Pandas DataFrames
- [ ] Loads profile configuration correctly
- [ ] Creates REST catalog connection with bearer token
- [ ] Supports both append and overwrite modes
- [ ] Returns dict with all required keys (path, num_rows, storage_type, etc.)
- [ ] Connection properly closed in finally block
- [ ] Errors raised with clear messages

**Dependencies:** Tasks 1, 2, 3 (enum, model, type hints must exist)

**Assigned To:** feature-dev-backend-agent

**Agent Type:** backend-agent

**Parallel:** false

---

### 5. Update OfflineStore.__call__() for ICEBERG

**File**: `src/seeknal/featurestore/featurestore.py`

**Task**: Add ICEBERG case to __call__ method

```python
def __call__(self, df, project_name):
    """Execute storage based on kind.

    Args:
        df: DataFrame to store
        project_name: Project name

    Returns:
        Storage result dict
    """
    if self.kind == OfflineStoreEnum.ICEBERG:
        return self._write_to_iceberg(df, project_name)
    elif self.kind == OfflineStoreEnum.FILE:
        # ... existing FILE logic ...
    elif self.kind == OfflineStoreEnum.HIVE_TABLE:
        # ... existing HIVE_TABLE logic ...
    else:
        raise ValueError(f"Unsupported store kind: {self.kind}")
```

**Acceptance Criteria:**
- [ ] ICEBERG kind calls `_write_to_iceberg()`
- [ ] Existing FILE and HIVE_TABLE logic unchanged
- [ ] Raises ValueError for unknown kinds
- [ ] No breaking changes to existing behavior

**Dependencies:** Task 4 (_write_to_iceberg must exist)

**Assigned To:** feature-dev-backend-agent

**Agent Type:** backend-agent

**Parallel:** false

---

### 6. Update FeatureGroup.write() Metadata Capture

**File**: `src/seeknal/featurestore/feature_group.py`

**Task**: Capture Iceberg snapshot metadata in offline_watermarks

```python
# In write() method, after offline materialization:
if offline_mat_result.get("storage_type") == "iceberg":
    # Store Iceberg metadata
    self.offline_watermarks.append({
        "timestamp": datetime.now().isoformat(),
        "storage_type": "iceberg",
        "table": offline_mat_result.get("table"),
        "catalog": offline_mat_result.get("catalog"),
        "namespace": offline_mat_result.get("namespace"),
        "row_count": offline_mat_result.get("num_rows"),
        "path": offline_mat_result.get("path"),
    })
```

**Acceptance Criteria:**
- [ ] Iceberg watermarks include all relevant metadata
- [ ] Captured when storage_type == "iceberg"
- [ ] Existing watermark logic unchanged for FILE/HIVE_TABLE
- [ ] Watermarks accessible via `fg.offline_watermarks`

**Dependencies:** Task 5 (OfflineStore must return correct metadata)

**Assigned To:** feature-dev-backend-agent

**Agent Type:** backend-agent

**Parallel:** false

---

### 7. Update Public API Exports

**File**: `src/seeknal/featurestore/__init__.py`

**Task**: Export new types for public API

```python
from .featurestore import (
    # ... existing exports ...
    IcebergStoreOutput,  # NEW
)
```

**Acceptance Criteria:**
- [ ] `IcebergStoreOutput` exported from featurestore module
- [ ] Users can import: `from seeknal.featurestore import IcebergStoreOutput`
- [ ] No breaking changes to existing imports

**Dependencies:** Task 2 (IcebergStoreOutput must exist)

**Assigned To:** feature-dev-backend-agent

**Agent Type:** backend-agent

**Parallel:** false

---

### 8. Create Unit Tests

**File**: `tests/featurestore/test_iceberg_materialization.py` (NEW)

**Task**: Write comprehensive unit tests for Iceberg functionality

```python
import pytest
from dataclasses import asdict
from seeknal.featurestore import (
    OfflineStore,
    OfflineStoreEnum,
    IcebergStoreOutput,
)

def test_iceberg_enum_exists():
    """Test ICEBERG enum value exists."""
    assert OfflineStoreEnum.ICEBERG == "iceberg"

def test_iceberg_store_output_defaults():
    """Test IcebergStoreOutput has correct defaults."""
    config = IcebergStoreOutput(table="test_table")
    assert config.catalog == "lakekeeper"
    assert config.namespace == "default"
    assert config.table == "test_table"
    assert config.mode == "append"
    assert config.warehouse is None

def test_offline_store_accepts_iceberg():
    """Test OfflineStore accepts IcebergStoreOutput."""
    config = IcebergStoreOutput(
        catalog="lakekeeper",
        namespace="prod",
        table="my_features",
        mode="append"
    )
    store = OfflineStore(
        kind=OfflineStoreEnum.ICEBERG,
        value=config,
        name="test_store"
    )
    assert store.kind == OfflineStoreEnum.ICEBERG
    assert store.value == config

def test_iceberg_store_output_requires_table():
    """Test table field is required."""
    with pytest.raises(Exception):
        IcebergStoreOutput(table="")  # Empty table should fail

def test_iceberg_write_to_iceberg_method_exists():
    """Test _write_to_iceberg method exists."""
    store = OfflineStore(
        kind=OfflineStoreEnum.ICEBERG,
        value=IcebergStoreOutput(table="test"),
        name="test"
    )
    assert hasattr(store, '_write_to_iceberg')
    assert callable(store._write_to_iceberg)

# Mock integration tests (use mocks for catalog connection)
@pytest.mark.integration
def test_iceberg_write_with_mock_catalog(mocker):
    """Test Iceberg write path with mocked catalog."""
    # Mock DuckDB connection and operations
    mock_con = mocker.MagicMock()
    mocker.patch('duckdb.connect', return_value=mock_con)
    mocker.patch('seeknal.workflow.materialization.operations.DuckDBIcebergExtension')

    store = OfflineStore(
        kind=OfflineStoreEnum.ICEBERG,
        value=IcebergStoreOutput(
            catalog="mock_catalog",
            namespace="test_ns",
            table="test_table"
        ),
        name="test"
    )

    result = store._write_to_iceberg(test_df, "test_project")

    assert result["storage_type"] == "iceberg"
    assert result["table"] == "test_table"
    assert result["namespace"] == "test_ns"
```

**Acceptance Criteria:**
- [ ] Test file created with all test cases
- [ ] Tests enum, model, and method existence
- [ ] Mock integration tests for catalog operations
- [ ] Edge cases covered (empty table, invalid config)
- [ ] Tests run with: `pytest tests/featurestore/test_iceberg_materialization.py`

**Dependencies:** Tasks 1-7 (all implementation must be complete)

**Assigned To:** tactical-engineering:test-agent

**Agent Type:** test-agent

**Parallel:** true (can run parallel to Task 9)

---

### 9. Create Real Infrastructure Integration Tests

**File**: `tests/integration/test_iceberg_real.py` (NEW)

**Task**: Write integration tests that run against real MinIO + Lakekeeper on atlas-dev-server

**Infrastructure Details:**
- **Server**: atlas-dev-server (172.19.0.9)
- **MinIO**: Port 9000, S3-compatible storage
- **Lakekeeper**: Port 8181, REST catalog
- **Warehouse**: `s3://warehouse`
- **Access**: SSH fitra@atlas-dev-server

**Environment Setup:**
```bash
# Required environment variables for testing
export LAKEKEEPER_URI="http://172.19.0.9:8181"
export LAKEKEEPER_WAREHOUSE="s3://warehouse"
export LAKEKEEPER_WAREHOUSE_ID="c008ea5c-fb89-11f0-aa64-c32ca2f52144"
export AWS_ENDPOINT_URL="http://172.19.0.9:9000"
export AWS_REGION="us-east-1"
export AWS_ACCESS_KEY_ID="minioadmin"
export AWS_SECRET_ACCESS_KEY="minioadmin"
export KEYCLOAK_TOKEN_URL="http://172.19.0.9:8080/realms/atlas/protocol/openid-connect/token"
export KEYCLOAK_CLIENT_ID="duckdb"
export KEYCLOAK_CLIENT_SECRET="duckdb-secret-change-in-production"
```

**Test Implementation:**

```python
"""
Integration tests for Iceberg materialization with real infrastructure.

Tests run against MinIO + Lakekeeper deployed on atlas-dev-server.
These tests verify end-to-end functionality with actual Iceberg operations.

Prerequisites:
1. atlas-dev-server must be running with MinIO, Lakekeeper, PostgreSQL
2. Environment variables must be set (see above)
3. SSH access to fitra@atlas-dev-server

Run with: pytest tests/integration/test_iceberg_real.py -v --integration
"""

import os
import pytest
import pandas as pd
from datetime import datetime

from seeknal.featurestore import (
    FeatureGroup,
    Materialization,
    OfflineMaterialization,
    OfflineStore,
    OfflineStoreEnum,
    IcebergStoreOutput,
)
from seeknal.entity import Entity

# Skip tests if infrastructure is not available
pytestmark = pytest.mark.skipif(
    not all([
        os.getenv("LAKEKEEPER_URI"),
        os.getenv("AWS_ENDPOINT_URL"),
    ]),
    reason="Infrastructure not available (set LAKEKEEPER_URI, AWS_ENDPOINT_URL)"
)


@pytest.fixture
def sample_features_df():
    """Create sample features DataFrame for testing."""
    return pd.DataFrame({
        "customer_id": ["C001", "C002", "C003", "C004", "C005"],
        "total_orders": [10, 5, 20, 3, 15],
        "total_spend": [1500.50, 750.25, 3000.00, 450.75, 2250.00],
        "avg_order_value": [150.05, 150.05, 150.00, 150.25, 150.00],
        "last_order_date": pd.to_datetime([
            "2024-01-15", "2024-01-10", "2024-01-20", "2024-01-05", "2024-01-18"
        ]),
        "feature_event_time": pd.to_datetime([
            "2024-01-15", "2024-01-10", "2024-01-20", "2024-01-05", "2024-01-18"
        ]),
    })


@pytest.fixture
def customer_entity():
    """Create customer entity for testing."""
    return Entity(
        name="customer",
        join_keys=["customer_id"],
        description="Customer entity for Iceberg testing"
    )


@pytest.fixture
def iceberg_config():
    """Get Iceberg configuration from environment."""
    return IcebergStoreOutput(
        catalog=os.getenv("LAKEKEEPER_URI", "http://172.19.0.9:8181"),
        warehouse=os.getenv("LAKEKEEPER_WAREHOUSE", "s3://warehouse"),
        namespace="test_integration",
        table="test_customer_features",
        mode="overwrite"  # Use overwrite for tests to clean state
    )


@pytest.mark.integration
def test_create_feature_group_with_iceberg(
    sample_features_df,
    customer_entity,
    iceberg_config
):
    """Test creating a FeatureGroup with Iceberg storage."""
    fg = FeatureGroup(
        name="test_iceberg_features",
        entity=customer_entity,
        materialization=Materialization(
            event_time_col="feature_event_time",
            offline=True,
            offline_materialization=OfflineMaterialization(
                store=OfflineStore(
                    kind=OfflineStoreEnum.ICEBERG,
                    value=iceberg_config,
                    name="test_iceberg_store"
                ),
                mode="overwrite"
            )
        )
    )

    # Set features and materialize
    fg.set_dataframe(sample_features_df).set_features()
    result = fg.write(feature_start_time=datetime(2024, 1, 1))

    # Verify write succeeded
    assert result is not None
    assert len(fg.offline_watermarks) > 0

    # Verify watermark metadata
    watermark = fg.offline_watermarks[-1]
    assert watermark["storage_type"] == "iceberg"
    assert watermark["table"] == "test_customer_features"
    assert watermark["namespace"] == "test_integration"
    assert watermark["row_count"] == 5


@pytest.mark.integration
def test_iceberg_append_mode(
    sample_features_df,
    customer_entity
):
    """Test appending to existing Iceberg table."""
    iceberg_config = IcebergStoreOutput(
        catalog=os.getenv("LAKEKEEPER_URI", "http://172.19.0.9:8181"),
        warehouse=os.getenv("LAKEKEEPER_WAREHOUSE", "s3://warehouse"),
        namespace="test_integration",
        table="test_append_features",
        mode="append"
    )

    fg = FeatureGroup(
        name="test_append_features",
        entity=customer_entity,
        materialization=Materialization(
            event_time_col="feature_event_time",
            offline=True,
            offline_materialization=OfflineMaterialization(
                store=OfflineStore(
                    kind=OfflineStoreEnum.ICEBERG,
                    value=iceberg_config,
                    name="test_append_store"
                ),
                mode="append"
            )
        )
    )

    # First write
    fg.set_dataframe(sample_features_df).set_features()
    fg.write(feature_start_time=datetime(2024, 1, 1))
    first_count = fg.offline_watermarks[-1]["row_count"]

    # Second write (append)
    additional_df = pd.DataFrame({
        "customer_id": ["C006"],
        "total_orders": [8],
        "total_spend": [1200.00],
        "avg_order_value": [150.00],
        "last_order_date": pd.to_datetime(["2024-01-12"]),
        "feature_event_time": pd.to_datetime(["2024-01-12"]),
    })

    fg.set_dataframe(additional_df).set_features()
    fg.write(feature_start_time=datetime(2024, 1, 1))
    second_count = fg.offline_watermarks[-1]["row_count"]

    # Verify append happened (table should have more rows now)
    # Note: The returned row_count is for the current write only
    assert second_count == 1  # Only 1 row in second write


@pytest.mark.integration
def test_iceberg_overwrite_mode(
    sample_features_df,
    customer_entity
):
    """Test overwriting existing Iceberg table."""
    iceberg_config = IcebergStoreOutput(
        catalog=os.getenv("LAKEKEEPER_URI", "http://172.19.0.9:8181"),
        warehouse=os.getenv("LAKEKEEPER_WAREHOUSE", "s3://warehouse"),
        namespace="test_integration",
        table="test_overwrite_features",
        mode="overwrite"
    )

    fg = FeatureGroup(
        name="test_overwrite_features",
        entity=customer_entity,
        materialization=Materialization(
            event_time_col="feature_event_time",
            offline=True,
            offline_materialization=OfflineMaterialization(
                store=OfflineStore(
                    kind=OfflineStoreEnum.ICEBERG,
                    value=iceberg_config,
                    name="test_overwrite_store"
                ),
                mode="overwrite"
            )
        )
    )

    # First write
    fg.set_dataframe(sample_features_df).set_features()
    fg.write(feature_start_time=datetime(2024, 1, 1))
    first_watermark = len(fg.offline_watermarks)

    # Second write (should replace data)
    smaller_df = pd.DataFrame({
        "customer_id": ["C001"],
        "total_orders": [99],
        "total_spend": [9999.99],
        "avg_order_value": [101.01],
        "last_order_date": pd.to_datetime(["2024-01-01"]),
        "feature_event_time": pd.to_datetime(["2024-01-01"]),
    })

    fg.set_dataframe(smaller_df).set_features()
    fg.write(feature_start_time=datetime(2024, 1, 1))

    # Verify overwrite happened
    assert len(fg.offline_watermarks) == first_watermark + 1
    assert fg.offline_watermarks[-1]["row_count"] == 1


@pytest.mark.integration
def test_backward_compatibility_file_storage(
    sample_features_df,
    customer_entity
):
    """Test that FILE storage still works (backward compatibility)."""
    fg = FeatureGroup(
        name="test_file_features",
        entity=customer_entity,
        materialization=Materialization(
            event_time_col="feature_event_time",
            offline=True,
            offline_materialization=OfflineMaterialization(
                store=OfflineStore(
                    kind=OfflineStoreEnum.FILE,
                    value=FeatureStoreFileOutput(
                        path="/tmp/test_features",
                        kind=FileKindEnum.PARQUET
                    ),
                    name="test_file_store"
                ),
                mode="overwrite"
            )
        )
    )

    fg.set_dataframe(sample_features_df).set_features()
    result = fg.write(feature_start_time=datetime(2024, 1, 1))

    # Verify FILE storage still works
    assert result is not None
    assert fg.offline_watermarks[-1]["storage_type"] != "iceberg"


@pytest.mark.integration
def test_iceberg_with_spark_dataframe(customer_entity):
    """Test Iceberg write with Spark DataFrame (if available)."""
    try:
        from pyspark.sql import SparkSession
        spark = SparkSession.builder.getOrCreate()
    except ImportError:
        pytest.skip("PySpark not available")

    spark_df = spark.createDataFrame([
        ("S001", 5, 500.0, "2024-01-01"),
        ("S002", 10, 1000.0, "2024-01-02"),
    ], ["customer_id", "total_orders", "total_spend", "event_date"])

    iceberg_config = IcebergStoreOutput(
        catalog=os.getenv("LAKEKEEPER_URI", "http://172.19.0.9:8181"),
        warehouse=os.getenv("LAKEKEEPER_WAREHOUSE", "s3://warehouse"),
        namespace="test_integration",
        table="test_spark_features",
        mode="overwrite"
    )

    fg = FeatureGroup(
        name="test_spark_iceberg",
        entity=customer_entity,
        materialization=Materialization(
            event_time_col="event_date",
            offline=True,
            offline_materialization=OfflineMaterialization(
                store=OfflineStore(
                    kind=OfflineStoreEnum.ICEBERG,
                    value=iceberg_config,
                    name="test_spark_store"
                ),
                mode="overwrite"
            )
        )
    )

    fg.set_dataframe(spark_df).set_features()
    result = fg.write(feature_start_time=datetime(2024, 1, 1))

    # Verify Spark DataFrame was converted and written
    assert result is not None
    assert len(fg.offline_watermarks) > 0


@pytest.mark.integration
def test_verify_table_accessible_via_duckdb(iceberg_config):
    """Verify that written table is accessible via DuckDB."""
    import duckdb
    from seeknal.workflow.materialization.operations import DuckDBIcebergExtension

    # Create connection and load extension
    con = duckdb.connect(":memory:")
    DuckDBIcebergExtension.load_extension(con)

    # Setup catalog
    catalog_uri = os.getenv("LAKEKEEPER_URI", "http://172.19.0.9:8181")
    warehouse = os.getenv("LAKEKEEPER_WAREHOUSE", "s3://warehouse")

    # Note: This test assumes a table was created by previous tests
    # In practice, you'd set up a test table first

    # For now, just verify we can connect to the catalog
    # TODO: Add table verification after test data setup
    con.close()
```

**Acceptance Criteria:**
- [ ] Integration test file created
- [ ] Tests run against real MinIO + Lakekeeper on atlas-dev-server
- [ ] Tests cover create, append, overwrite modes
- [ ] Tests verify backward compatibility with FILE storage
- [ ] Tests handle both Pandas and Spark DataFrames
- [ ] Environment variable validation in tests
- [ ] Tests can be run with: `pytest tests/integration/test_iceberg_real.py -v --integration`
- [ ] Cleanup/teardown logic to remove test tables

**Dependencies:** Tasks 1-7 (all implementation must be complete)

**Assigned To:** tactical-engineering:test-agent

**Agent Type:** test-agent

**Parallel:** true (can run parallel to Task 8)

---

### 10. Create Example Notebook

**File**: `examples/iceberg_feature_group.py` (NEW)

**Task**: Create example demonstrating Iceberg Feature Group usage

```python
"""
Example: Using Iceberg Storage for Feature Groups

This example demonstrates how to use Apache Iceberg as the storage
backend for Seeknal Feature Groups, enabling:
- ACID transactions for feature writes
- Time travel queries
- Schema evolution
- Cloud storage (S3, GCS, Azure)

Prerequisites:
1. Lakekeeper REST catalog running
2. MinIO or other S3-compatible storage
3. Environment variables configured (see below)

Environment Variables:
    export LAKEKEEPER_URI="http://your-lakekeeper:8181"
    export LAKEKEEPER_WAREHOUSE="s3://your-bucket/warehouse"
    export AWS_ENDPOINT_URL="http://your-minio:9000"
    export AWS_ACCESS_KEY_ID="your-access-key"
    export AWS_SECRET_ACCESS_KEY="your-secret-key"
"""

from datetime import datetime
import pandas as pd

from seeknal.featurestore import (
    FeatureGroup,
    Materialization,
    OfflineMaterialization,
    OfflineStore,
    OfflineStoreEnum,
    IcebergStoreOutput,
)
from seeknal.entity import Entity

# ============================================================
# Step 1: Create Entity
# ============================================================
customer_entity = Entity(
    name="customer",
    join_keys=["customer_id"],
    description="Customer entity for e-commerce features"
)

# ============================================================
# Step 2: Prepare Sample Data
# ============================================================
# Sample customer features
customer_features_df = pd.DataFrame({
    "customer_id": [
        "C001", "C002", "C003", "C004", "C005",
        "C006", "C007", "C008", "C009", "C010"
    ],
    "total_orders": [15, 8, 25, 3, 12, 20, 5, 30, 2, 18],
    "total_spend": [
        2250.50, 1200.25, 3750.00, 450.75, 1800.00,
        3000.25, 750.50, 4500.00, 300.25, 2700.00
    ],
    "avg_order_value": [
        150.03, 150.03, 150.00, 150.25, 150.00,
        150.01, 150.10, 150.00, 150.12, 150.00
    ],
    "days_since_last_order": [5, 12, 2, 45, 8, 3, 30, 1, 60, 7],
    "customer_segment": [
        "VIP", "Regular", "VIP", "Inactive", "Regular",
        "VIP", "Regular", "VIP", "Inactive", "Regular"
    ],
    "feature_event_time": pd.to_datetime([
        "2024-01-15", "2024-01-15", "2024-01-15", "2024-01-15", "2024-01-15",
        "2024-01-15", "2024-01-15", "2024-01-15", "2024-01-15", "2024-01-15"
    ]),
})

print("Sample features:")
print(customer_features_df.head())

# ============================================================
# Step 3: Create Feature Group with Iceberg Storage
# ============================================================

# Configure Iceberg storage
iceberg_config = IcebergStoreOutput(
    catalog="lakekeeper",           # Catalog from profiles.yml
    warehouse="s3://warehouse",      # S3 bucket/path
    namespace="prod",                # Iceberg namespace (database)
    table="customer_features",       # Table name
    mode="append"                    # Write mode: append or overwrite
)

# Create FeatureGroup
fg = FeatureGroup(
    name="customer_features_iceberg",
    entity=customer_entity,
    description="Customer behavioral features with Iceberg storage",
    materialization=Materialization(
        event_time_col="feature_event_time",
        offline=True,
        offline_materialization=OfflineMaterialization(
            store=OfflineStore(
                kind=OfflineStoreEnum.ICEBERG,  # Use Iceberg!
                value=iceberg_config,
                name="customer_iceberg_store"
            ),
            mode="append"
        )
    )
)

# Set features from DataFrame
fg.set_dataframe(customer_features_df).set_features()

print(f"\nFeature Group: {fg.name}")
print(f"Features: {[f.name for f in fg.features]}")

# ============================================================
# Step 4: Write to Iceberg
# ============================================================

print("\nWriting features to Iceberg...")

result = fg.write(
    feature_start_time=datetime(2024, 1, 1),
    feature_end_time=datetime(2024, 1, 31)
)

print(f"âœ“ Write complete!")
print(f"  Storage type: {fg.offline_watermarks[-1]['storage_type']}")
print(f"  Table: {fg.offline_watermarks[-1]['table']}")
print(f"  Namespace: {fg.offline_watermarks[-1]['namespace']}")
print(f"  Rows written: {fg.offline_watermarks[-1]['row_count']}")

# ============================================================
# Step 5: Query from DuckDB (Verify)
# ============================================================

print("\nVerifying with DuckDB query...")

import duckdb
from seeknal.workflow.materialization.operations import DuckDBIcebergExtension

con = duckdb.connect(":memory:")
DuckDBIcebergExtension.load_extension(con)

# Setup catalog (same as write)
# Note: In production, use profiles.yml for credentials
catalog_uri = "http://your-lakekeeper:8181"
warehouse = "s3://warehouse"
bearer_token = "your-token"

DuckDBIcebergExtension.create_rest_catalog(
    con=con,
    catalog_name="prod_catalog",
    uri=catalog_uri,
    warehouse_path=warehouse,
    bearer_token=bearer_token,
)

# Query the table
query_result = con.execute(f"""
    SELECT
        customer_segment,
        COUNT(*) as customer_count,
        AVG(total_spend) as avg_spend
    FROM prod_catalog.prod.customer_features
    GROUP BY customer_segment
    ORDER BY avg_spend DESC
""").fetchall()

print("\nCustomer segment summary:")
for row in query_result:
    print(f"  {row[0]}: {row[1]} customers, ${row[2]:.2f} avg spend")

con.close()

# ============================================================
# Step 6: Append More Data
# ============================================================

print("\nAppending additional features...")

# New day of data
new_features_df = pd.DataFrame({
    "customer_id": ["C001", "C002", "C011"],  # C011 is new!
    "total_orders": [16, 9, 1],  # Incremented for C001, C002
    "total_spend": [2400.50, 1350.25, 150.00],
    "avg_order_value": [150.03, 150.03, 150.00],
    "days_since_last_order": [1, 6, 0],
    "customer_segment": ["VIP", "Regular", "New"],
    "feature_event_time": pd.to_datetime([
        "2024-01-16", "2024-01-16", "2024-01-16"
    ]),
})

fg.set_dataframe(new_features_df).set_features()
fg.write(feature_start_time=datetime(2024, 1, 16))

print(f"âœ“ Append complete!")
print(f"  Total watermarks: {len(fg.offline_watermarks)}")

# ============================================================
# Bonus: Time Travel Query (via Lakekeeper)
# ============================================================

print("\nðŸ’¡ Tip: With Iceberg, you can query historical snapshots!")
print("   Example: Query features as of a specific timestamp")
print("   SELECT * FROM prod_catalog.prod.customer_features")
print("   FOR SYSTEM_TIME AS OF '2024-01-15 00:00:00'")
```

**Acceptance Criteria:**
- [ ] Example file created with complete working code
- [ ] Demonstrates create, write, append operations
- [ ] Shows verification query with DuckDB
- [ ] Includes environment variable documentation
- [ ] Code is well-commented and runnable
- [ ] Example can be executed as: `python examples/iceberg_feature_group.py`

**Dependencies:** Tasks 1-7 (implementation complete)

**Assigned To:** docs-agent

**Agent Type:** docs-agent

**Parallel:** true (can run parallel to Tasks 8, 9)

---

### 11. Update Documentation

**Files**: Multiple documentation files

**Task**: Update docs to include Iceberg examples

**Files to Update:**

1. **`docs/getting-started-comprehensive.md`**
   - Add section on Iceberg storage option
   - Include environment setup instructions
   - Show comparison of storage backends

2. **`docs/examples/featurestore.md`**
   - Add Iceberg examples alongside existing FILE/HIVE_TABLE examples
   - Include when to use each backend

3. **`README.md`**
   - Update DuckDB Integration section with Iceberg note
   - Link to Iceberg example

**Content to Add:**

```markdown
## Iceberg Storage for Feature Groups

Seeknal Feature Groups support Apache Iceberg as a storage backend,
providing ACID transactions, time travel, and cloud storage.

### Configuration

Set environment variables:
```bash
export LAKEKEEPER_URI="http://your-lakekeeper:8181"
export LAKEKEEPER_WAREHOUSE="s3://your-bucket/warehouse"
export AWS_ENDPOINT_URL="http://your-minio:9000"
```

### Usage

```python
from seeknal.featurestore import (
    FeatureGroup,
    OfflineStoreEnum,
    IcebergStoreOutput,
)

fg = FeatureGroup(
    name="my_features",
    entity=my_entity,
    materialization=Materialization(
        offline=True,
        offline_materialization=OfflineMaterialization(
            store=OfflineStore(
                kind=OfflineStoreEnum.ICEBERG,
                value=IcebergStoreOutput(
                    catalog="lakekeeper",
                    namespace="prod",
                    table="my_features"
                )
            )
        )
    )
)

fg.write()  # Writes to Iceberg!
```

### When to Use Iceberg

- **Team collaboration**: Share features across multiple users/systems
- **ACID requirements**: Need transactional guarantees
- **Time travel**: Query features as of historical points
- **Cloud storage**: Store in S3/GCS/Azure instead of local filesystem
- **Multiple compute engines**: Query from DuckDB, Spark, Trino

See `examples/iceberg_feature_group.py` for complete example.
```

**Acceptance Criteria:**
- [ ] All three docs files updated
- [ ] Environment setup instructions included
- [ ] Code examples are accurate and tested
- [ ] Cross-references between documents work
- [ ] Links to example file included

**Dependencies:** Tasks 1-7, 10 (implementation and example complete)

**Assigned To:** docs-agent

**Agent Type:** docs-agent

**Parallel:** true (can run parallel to Tasks 8, 9)

---

### 12. Run Real Infrastructure Tests

**Task**: Execute integration tests against atlas-dev-server and verify functionality

**Steps:**

1. **Verify Infrastructure Status**
   ```bash
   ssh fitra@atlas-dev-server "docker ps --filter name=minio --format '{{.Names}}'"
   ssh fitra@atlas-dev-server "docker ps --filter name=lakekeeper --format '{{.Names}}'"
   ```

2. **Set Environment Variables**
   ```bash
   export LAKEKEEPER_URI="http://172.19.0.9:8181"
   export LAKEKEEPER_WAREHOUSE="s3://warehouse"
   export LAKEKEEPER_WAREHOUSE_ID="c008ea5c-fb89-11f0-aa64-c32ca2f52144"
   export AWS_ENDPOINT_URL="http://172.19.0.9:9000"
   export AWS_REGION="us-east-1"
   export AWS_ACCESS_KEY_ID="minioadmin"
   export AWS_SECRET_ACCESS_KEY="minioadmin"
   export KEYCLOAK_TOKEN_URL="http://172.19.0.9:8080/realms/atlas/protocol/openid-connect/token"
   export KEYCLOAK_CLIENT_ID="duckdb"
   export KEYCLOAK_CLIENT_SECRET="duckdb-secret-change-in-production"
   ```

3. **Run Integration Tests**
   ```bash
   pytest tests/integration/test_iceberg_real.py -v --integration -s
   ```

4. **Verify with Manual Query**
   ```bash
   python -c "
   import duckdb
   con = duckdb.connect(':memory:')
   con.execute('INSTALL iceberg; LOAD iceberg;')
   # Setup catalog and query test table
   # ...
   "
   ```

5. **Clean Up Test Tables**
   ```bash
   ssh fitra@atlas-dev-server "mc rm --recursive myminio/warehouse/test_integration/"
   ```

**Acceptance Criteria:**
- [ ] Infrastructure (MinIO, Lakekeeper) is running
- [ ] All integration tests pass
- [ ] Manual query confirms table exists and has data
- [ ] Test tables cleaned up after verification
- [ ] Performance benchmarks collected (write times, row counts)

**Dependencies:** Tasks 1-11 (all implementation and tests complete)

**Assigned To:** qa-verification-agent

**Agent Type:** test-agent

**Parallel:** false

---

## Acceptance Criteria

### Functional Requirements

- [x] `OfflineStoreEnum.ICEBERG` added as new storage option
- [x] `IcebergStoreOutput` configuration model created
- [x] `OfflineStore.__call__()` implements Iceberg write path
- [x] `FeatureGroup.write()` supports Iceberg materialization
- [x] YAML feature groups can use Iceberg via `materialization` config
- [x] Python API works with Iceberg (no breaking changes)
- [x] Snapshot metadata captured in `offline_watermarks`

### Non-Functional Requirements

- [x] Reuses existing Iceberg infrastructure (no duplication)
- [x] Backward compatible (existing code continues to work)
- [x] Consistent with YAML pipeline materialization
- [x] Works with DuckDB and Spark DataFrames
- [x] Error handling for catalog connection failures
- [x] Integration tests pass against real infrastructure

### Quality Gates

- [x] Unit tests for Iceberg write path
- [x] Integration test with Lakekeeper catalog on atlas-dev-server
- [x] Documentation updated with Iceberg examples
- [x] Example notebook demonstrating usage
- [x] Performance benchmarks collected

## Team Orchestration

As the team lead, you have access to powerful tools for coordinating work across multiple agents. You NEVER write code directly - you orchestrate team members using these tools.

### Task Management Tools

**TaskCreate** - Create tasks in the shared task list:

```typescript
TaskCreate({
  subject: "Implement user authentication",
  description: "Create login/logout endpoints with JWT tokens. See specs/auth-plan.md for details.",
  activeForm: "Implementing authentication" // Shows in UI spinner when in_progress
})
// Returns: taskId (e.g., "1")
```

**TaskUpdate** - Update task status, assignment, or dependencies:

```typescript
TaskUpdate({
  taskId: "1",
  status: "in_progress", // pending â†’ in_progress â†’ completed
  owner: "builder-auth" // Assign to specific team member
})
```

**TaskList** - View all tasks and their status:

```typescript
TaskList({})
// Returns: Array of tasks with id, subject, status, owner, blockedBy
```

**TaskGet** - Get full details of a specific task:

```typescript
TaskGet({ taskId: "1" })
// Returns: Full task including description
```

### Task Dependencies

Use `addBlockedBy` to create sequential dependencies - blocked tasks cannot start until dependencies complete:

```typescript
// Task 2 depends on Task 1
TaskUpdate({
  taskId: "2",
  addBlockedBy: ["1"] // Task 2 blocked until Task 1 completes
})

// Task 3 depends on both Task 1 and Task 2
TaskUpdate({
  taskId: "3",
  addBlockedBy: ["1", "2"]
})
```

Dependency chain example:
```
Task 1: Setup foundation â†’ no dependencies
Task 2: Implement feature â†’ blockedBy: ["1"]
Task 3: Write tests â†’ blockedBy: ["2"]
Task 4: Final validation â†’ blockedBy: ["1", "2", "3"]
```

### Owner Assignment

Assign tasks to specific team members for clear accountability:

```typescript
// Assign task to a specific builder
TaskUpdate({
  taskId: "1",
  owner: "builder-api"
})

// Team members check for their assignments
TaskList({}) // Filter by owner to find assigned work
```

### Agent Deployment with Task Tool

**Task** - Deploy an agent to do work:

```typescript
Task({
  description: "Implement auth endpoints",
  prompt: "Implement the authentication endpoints as specified in Task 1...",
  subagent_type: "general-purpose",
  model: "opus", // or "opus" for complex work, "haiku" for VERY simple
  run_in_background: false // true for parallel execution
})
// Returns: agentId (e.g., "a1b2c3")
```

### Resume Pattern

Store the agentId to continue an agent's work with preserved context:

```typescript
// First deployment - agent works on initial task
Task({
  description: "Build user service",
  prompt: "Create the user service with CRUD operations...",
  subagent_type: "general-purpose"
})
// Returns: agentId: "abc123"

// Later - resume SAME agent with full context preserved
Task({
  description: "Continue user service",
  prompt: "Now add input validation to the endpoints you created...",
  subagent_type: "general-purpose",
  resume: "abc123" // Continues with previous context
})
```

**When to resume vs start fresh:**
- **Resume**: Continuing related work, agent needs prior context
- **Fresh**: Unrelated task, clean slate preferred

### Parallel Execution

Run multiple agents simultaneously with `run_in_background: true`:

```typescript
// Launch multiple agents in parallel
Task({
  description: "Build API endpoints",
  prompt: "...",
  subagent_type: "general-purpose",
  run_in_background: true
})
// Returns immediately with agentId and output_file path

Task({
  description: "Build frontend components",
  prompt: "...",
  subagent_type: "general-purpose",
  run_in_background: true
})

// Both agents now working simultaneously

// Check on progress
TaskOutput({
  task_id: "agentId",
  block: false, // non-blocking check
  timeout: 5000
})

// Wait for completion
TaskOutput({
  task_id: "agentId",
  block: true, // blocks until done
  timeout: 300000
})
```

### Orchestration Workflow

1. **Create tasks** with `TaskCreate` for each step in the plan
2. **Set dependencies** with `TaskUpdate` + `addBlockedBy`
3. **Assign owners** with `TaskUpdate` + `owner`
4. **Deploy agents** with `Task` to execute assigned work
5. **Monitor progress** with `TaskList` and `TaskOutput`
6. **Resume agents** with `Task` + `resume` for follow-up work
7. **Mark complete** with `TaskUpdate` + `status: "completed"`

### Team Members

#### Backend Implementation Team

**feature-dev-backend**
- **Name**: feature-dev-backend
- **Role**: Backend implementation
- **Agent Type**: feature-dev:code-explorer
- **Resume**: false
- **Responsibilities**: Core implementation (Tasks 1-7)

#### Testing Team

**integration-tester**
- **Name**: integration-tester
- **Role**: Integration testing
- **Agent Type**: tactical-engineering:test-agent
- **Resume**: false
- **Responsibilities**: Real infrastructure tests (Task 9)

**unit-tester**
- **Name**: unit-tester
- **Role**: Unit testing
- **Agent Type**: tactical-engineering:test-agent
- **Resume**: false
- **Responsibilities**: Unit tests (Task 8)

#### Documentation Team

**docs-writer**
- **Name**: docs-writer
- **Role**: Documentation
- **Agent Type**: docs-agent
- **Resume**: false
- **Responsibilities**: Examples and docs (Tasks 10-11)

#### QA Team

**qa-verifier**
- **Name**: qa-verifier
- **Role**: QA verification
- **Agent Type**: qa-verification-agent
- **Resume**: false
- **Responsibilities**: Final validation (Task 12)

## Alternative Approaches Considered

### Option A: Separate IcebergFeatureGroup Class
**Description**: Create a new `IcebergFeatureGroup` class inheriting from `FeatureGroup`

**Pros**:
- Clean separation of concerns
- No risk of breaking existing code

**Cons**:
- Code duplication
- Users need to know which class to use
- Inconsistent API

**Decision**: Rejected - adds complexity without clear benefit

### Option B: Modify Only YAML Pipeline Integration
**Description**: Only support Iceberg for YAML-defined feature groups

**Pros**:
- Simpler implementation
- Python API unchanged

**Cons**:
- ML engineers who use Python API can't use Iceberg
- Inconsistent feature between YAML and Python

**Decision**: Rejected - limits functionality for primary user group (ML engineers)

### Option C: Use External Library
**Description**: Use a separate library (like `iceberg-python`) for Iceberg operations

**Pros**:
- Leverages existing implementation
- Less custom code

**Cons**:
- Additional dependency
- May not align with existing YAML pipeline approach
- Version management complexity

**Decision**: Rejected - existing infrastructure is sufficient and consistent

## Success Metrics

| Metric | Current | Target | Measurement |
|--------|---------|--------|-------------|
| Storage backend options | 2 (FILE, HIVE_TABLE) | 3 (add ICEBERG) | Code enum count |
| Features accessible from multiple engines | 1 (DuckDB only via Parquet) | 4 (DuckDB, Spark, Trino, Flink) | Engine compatibility |
| Time travel capability | No | Yes | Query historical snapshots |
| ACID transaction support | No | Yes | Concurrent write safety |
| Cloud storage support | Manual | Native (S3/GCS/Azure) | Storage location flexibility |

## Dependencies & Prerequisites

### External Dependencies

| Dependency | Version | Purpose | Risk |
|------------|---------|---------|------|
| DuckDB Iceberg Extension | Latest | Iceberg table operations | Low - stable extension |
| Lakekeeper REST Catalog | Beta | Catalog service | Medium - API changes possible |
| MinIO | Latest | S3-compatible storage | Low - standard S3 API |
| PostgreSQL (Lakekeeper backing) | 14+ | Catalog metadata | Low - standard SQL |

### Internal Dependencies

| Dependency | Status | Notes |
|------------|--------|-------|
| YAML Pipeline Iceberg Infrastructure | âœ… Complete | Reused for Feature Groups |
| ProfileLoader | âœ… Complete | Loads catalog credentials |
| DuckDBIcebergExtension | âœ… Complete | DuckDB Iceberg operations |
| OfflineStore (existing) | âœ… Complete | Extended with ICEBERG case |

### Infrastructure Requirements

| Component | Host | Port | Purpose |
|-----------|------|------|---------|
| MinIO | atlas-dev-server | 9000 | S3-compatible object storage |
| Lakekeeper | atlas-dev-server | 8181 | REST catalog service |
| PostgreSQL | atlas-dev-server | 5432 | Catalog metadata storage |

**Access**: SSH `fitra@atlas-dev-server` or use IP `172.19.0.9`

## Risk Analysis & Mitigation

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| DuckDB Iceberg extension bugs | Low | High | Comprehensive testing, fallback to Parquet |
| Lakekeeper API changes | Low | Medium | Version pinning, abstract catalog operations |
| Performance degradation | Low | High | Benchmarking, optimize write batch sizes |
| Breaking existing API | Low | High | Backward compatibility tests, opt-in feature |
| SSH access to dev server | Low | Medium | Document setup, provide alternative endpoints |

## Resource Requirements

### Development Time Estimate

| Phase | Complexity | Estimate |
|-------|------------|----------|
| Phase 1: Core Implementation (Tasks 1-7) | Medium | 2-3 days |
| Phase 2: Testing (Tasks 8-9) | Medium | 1-2 days |
| Phase 3: Documentation (Tasks 10-11) | Low | 1 day |
| Phase 4: Verification (Task 12) | Low | 0.5 day |
| **Total** | | **4.5-6.5 days** |

### Server Resources

| Resource | Quantity | Purpose |
|----------|----------|---------|
| atlas-dev-server | 1 | Integration testing environment |
| MinIO container | 1 | S3-compatible storage |
| Lakekeeper container | 1 | REST catalog |
| PostgreSQL container | 1 | Catalog metadata |

## Documentation Plan

| Document | Location | When |
|----------|----------|------|
| Integration tests | `tests/integration/test_iceberg_real.py` | Phase 2 |
| Example notebook | `examples/iceberg_feature_group.py` | Phase 3 |
| Getting started guide | `docs/getting-started-comprehensive.md` | Phase 3 |
| Feature store examples | `docs/examples/featurestore.md` | Phase 3 |
| README update | `README.md` | Phase 3 |

## Validation Commands

### Infrastructure Validation

```bash
# Check MinIO is running
ssh fitra@atlas-dev-server "docker ps | grep minio"

# Check Lakekeeper is running
ssh fitra@atlas-dev-server "docker ps | grep lakekeeper"

# Verify connectivity
curl -s http://172.19.0.9:8181/health || echo "Lakekeeper not accessible"
```

### Unit Tests

```bash
# Run all unit tests
pytest tests/featurestore/test_iceberg_materialization.py -v

# Run specific test
pytest tests/featurestore/test_iceberg_materialization.py::test_iceberg_enum_exists -v
```

### Integration Tests

```bash
# Set environment variables
export LAKEKEEPER_URI="http://172.19.0.9:8181"
export LAKEKEEPER_WAREHOUSE="s3://warehouse"
export AWS_ENDPOINT_URL="http://172.19.0.9:9000"
export AWS_ACCESS_KEY_ID="minioadmin"
export AWS_SECRET_ACCESS_KEY="minioadmin"

# Run integration tests
pytest tests/integration/test_iceberg_real.py -v --integration -s
```

### Manual Verification

```bash
# Run example script
python examples/iceberg_feature_group.py

# Verify table exists with DuckDB
python -c "
import duckdb
con = duckdb.connect(':memory:')
con.install_extension('iceberg')
con.load_extension('iceberg')
# Query test table...
"
```

### Cleanup

```bash
# Remove test tables from MinIO
ssh fitra@atlas-dev-server "mc alias set myminio http://localhost:9000 minioadmin minioadmin"
ssh fitra@atlas-dev-server "mc rm --recursive myminio/warehouse/test_integration/"
```

## Notes

### Implementation Notes

1. **Reuse Infrastructure**: All Iceberg operations reuse existing YAML pipeline infrastructure from `src/seeknal/workflow/materialization/`

2. **No New CLI Commands**: Iceberg materialization works through existing `FeatureGroup.write()` API

3. **Environment-Based Configuration**: Catalog credentials loaded from environment variables or `profiles.yml`

4. **DataFrame Conversion**: Spark DataFrames automatically converted to Pandas for Iceberg writes

5. **Schema Evolution**: Supported through Iceberg's native capabilities (no additional code needed)

### Testing Notes

1. **Real Infrastructure**: Integration tests run against actual MinIO + Lakekeeper on atlas-dev-server

2. **Environment Variables Required**: Tests require `LAKEKEEPER_URI` and related env vars

3. **Namespace Isolation**: Tests use `test_integration` namespace to avoid conflicts

4. **Cleanup**: Test tables should be cleaned up after verification

### Migration Notes

1. **No Migration Required**: Existing Feature Groups continue to work without changes

2. **Opt-In Feature**: Users explicitly choose ICEBERG storage via configuration

3. **Dual Storage Possible**: Can write to both FILE and ICEBERG if needed

---

## Checklist Summary

### Phase 1: Core Implementation ðŸŸ¢
- [x] 1. Add ICEBERG to OfflineStoreEnum
- [x] 2. Create IcebergStoreOutput Model
- [x] 3. Update OfflineStore Type Hints
- [x] 4. Implement _write_to_iceberg() Method
- [x] 5. Update OfflineStore.__call__() for ICEBERG
- [x] 6. Update FeatureGroup.write() Metadata Capture
- [x] 7. Update Public API Exports

### Phase 2: Testing ðŸŸ¢
- [x] 8. Create Unit Tests
- [x] 9. Create Real Infrastructure Integration Tests

### Phase 3: Documentation ðŸŸ¢
- [x] 10. Create Example Notebook
- [x] 11. Update Documentation

### Phase 4: Verification ðŸŸ¢
- [x] 12. Run Real Infrastructure Tests
