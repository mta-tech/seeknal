{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iceberg Feature Groups in Seeknal\n",
    "\n",
    "This notebook demonstrates how to use **Apache Iceberg** as the storage backend for Seeknal Feature Groups.\n",
    "\n",
    "## What is Apache Iceberg?\n",
    "\n",
    "**Apache Iceberg** is an open-source table format for huge analytic datasets. It provides:\n",
    "\n",
    "- **ACID Transactions**: Atomic writes with automatic rollback\n",
    "- **Time Travel**: Query features as of any point in time\n",
    "- **Schema Evolution**: Add/modify features without rewrites\n",
    "- **Cloud Storage**: Native support for S3, GCS, and Azure Blob\n",
    "- **Compatibility**: Works with DuckDB, Spark, Trino, and more\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before running this notebook, ensure:\n",
    "\n",
    "1. **Seeknal is installed**: `pip install seeknal`\n",
    "2. **DuckDB is available**: Comes with Seeknal\n",
    "3. **REST Catalog is running**: e.g., Lakekeeper at `http://localhost:8181`\n",
    "4. **Storage is configured**: S3 bucket or local filesystem\n",
    "\n",
    "### Configure Catalog\n",
    "\n",
    "Create or update `~/.seeknal/profiles.yml`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show example profile configuration\n",
    "profile_content = \"\"\"\n",
    "materialization:\n",
    "  catalog:\n",
    "    uri: http://localhost:8181  # Lakekeeper REST catalog\n",
    "    warehouse: s3://my-bucket/warehouse\n",
    "    bearer_token: optional_token  # If auth required\n",
    "\"\"\"\n",
    "\n",
    "print(profile_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Initialize Project and Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "\n",
    "# Set environment variables (alternative to profiles.yml)\n",
    "os.environ['LAKEKEEPER_URI'] = 'http://localhost:8181'\n",
    "os.environ['LAKEKEEPER_WAREHOUSE'] = 's3://iceberg/warehouse'\n",
    "\n",
    "print(\"Environment configured for Iceberg storage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Feature Groups with Iceberg Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seeknal.featurestore import (\n",
    "    FeatureGroup,\n",
    "    Materialization,\n",
    "    OfflineMaterialization,\n",
    "    OfflineStore,\n",
    "    OfflineStoreEnum,\n",
    "    IcebergStoreOutput,\n",
    ")\n",
    "from seeknal.entity import Entity\n",
    "\n",
    "# Create an entity (defines the join key)\n",
    "customer_entity = Entity(\n",
    "    name=\"customer\",\n",
    "    join_keys=[\"customer_id\"]\n",
    ")\n",
    "\n",
    "print(\"Entity created:\", customer_entity.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample feature data\n",
    "features_df = pd.DataFrame({\n",
    "    \"customer_id\": [\"A001\", \"A002\", \"A003\", \"A001\", \"A002\"],\n",
    "    \"event_date\": [\"2024-01-01\", \"2024-01-01\", \"2024-01-01\", \"2024-01-02\", \"2024-01-02\"],\n",
    "    \"total_orders\": [5, 10, 3, 2, 5],\n",
    "    \"total_spend\": [100.0, 250.0, 75.0, 50.0, 125.0],\n",
    "    \"avg_order_value\": [20.0, 25.0, 25.0, 25.0, 25.0],\n",
    "    \"days_since_last_order\": [0, 0, 0, 1, 1],\n",
    "})\n",
    "\n",
    "print(\"Sample features:\")\n",
    "print(features_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Feature Group with Iceberg storage\n",
    "fg = FeatureGroup(\n",
    "    name=\"customer_features\",\n",
    "    entity=customer_entity,\n",
    "    materialization=Materialization(\n",
    "        event_time_col=\"event_date\",\n",
    "        offline=True,\n",
    "        offline_materialization=OfflineMaterialization(\n",
    "            store=OfflineStore(\n",
    "                kind=OfflineStoreEnum.ICEBERG,  # Use Iceberg!\n",
    "                value=IcebergStoreOutput(\n",
    "                    catalog=\"lakekeeper\",        # Catalog name\n",
    "                    namespace=\"prod\",            # Namespace (database)\n",
    "                    table=\"customer_features\",   # Table name\n",
    "                    mode=\"append\"               # Write mode\n",
    "                )\n",
    "            ),\n",
    "            mode=\"append\"\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Feature Group created with Iceberg storage\")\n",
    "print(f\"Storage kind: {fg.materialization.offline_materialization.store.kind}\")\n",
    "print(f\"Table: {fg.materialization.offline_materialization.store.value.catalog}.\")\n",
    "print(f\"      {fg.materialization.offline_materialization.store.value.namespace}.\")\n",
    "print(f\"      {fg.materialization.offline_materialization.store.value.table}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Features to Iceberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the DataFrame and define features\n",
    "fg.set_dataframe(features_df).set_features()\n",
    "\n",
    "# Write to Iceberg table\n",
    "result = fg.write(feature_start_time=datetime(2024, 1, 1))\n",
    "\n",
    "print(\"Features written to Iceberg table!\")\n",
    "print(f\"\\nResult:\")\n",
    "print(f\"  Path: {result['path']}\")\n",
    "print(f\"  Rows: {result['num_rows']}\")\n",
    "print(f\"  Storage: {result['storage_type']}\")\n",
    "print(f\"  Snapshot ID: {result['snapshot_id'][:16]}...\")  # First 16 chars\n",
    "print(f\"  Table: {result['table']}\")\n",
    "print(f\"  Namespace: {result['namespace']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying Iceberg Tables with DuckDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DuckDB connection\n",
    "con = duckdb.connect(\":memory:\")\n",
    "\n",
    "# Load Iceberg extension\n",
    "con.install_extension(\"iceberg\")\n",
    "con.load_extension(\"iceberg\")\n",
    "\n",
    "print(\"DuckDB Iceberg extension loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach REST catalog\n",
    "catalog_uri = os.getenv('LAKEKEEPER_URI', 'http://localhost:8181')\n",
    "warehouse_path = os.getenv('LAKEKEEPER_WAREHOUSE', 's3://iceberg/warehouse')\n",
    "\n",
    "con.execute(f\"\"\"\n",
    "ATTACH '{catalog_uri}' AS seeknal_catalog (\n",
    "    TYPE iceberg,\n",
    "    WAREHOUSE '{warehouse_path}'\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "print(f\"Attached catalog: {catalog_uri}\")\n",
    "print(f\"Warehouse: {warehouse_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the Iceberg table\n",
    "query_result = con.execute(\"\"\"\n",
    "SELECT \n",
    "    customer_id,\n",
    "    total_orders,\n",
    "    total_spend,\n",
    "    avg_order_value\n",
    "FROM seeknal_catalog.prod.customer_features\n",
    "ORDER BY customer_id, event_date\n",
    "\"\"\").fetchall()\n",
    "\n",
    "print(\"\\nFeatures from Iceberg table:\")\n",
    "for row in query_result:\n",
    "    print(f\"  {row[0]}: orders={row[1]}, spend=${row[2]:.2f}, avg=${row[3]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Travel with Iceberg\n",
    "\n",
    "Iceberg allows querying data as of any snapshot. This is useful for:\n",
    "- Debugging model performance at specific times\n",
    "- Rolling back to previous feature versions\n",
    "- Auditing feature changes over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get snapshot history\n",
    "snapshots = con.execute(\"\"\"\n",
    "SELECT snapshot_id, committed_at\n",
    "FROM seeknal_catalog.prod.customer_features.snapshots\n",
    "ORDER BY committed_at DESC\n",
    "\"\"\").fetchall()\n",
    "\n",
    "print(\"\\nSnapshot history:\")\n",
    "for i, (snapshot_id, committed_at) in enumerate(snapshots[:5], 1):\n",
    "    print(f\"  {i}. {snapshot_id[:16]}... at {committed_at}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query as of specific snapshot\n",
    "if snapshots:\n",
    "    first_snapshot_id = snapshots[0][0]\n",
    "    \n",
    "    con.execute(f\"USE SNAPSHOT '{first_snapshot_id}'\")\n",
    "    \n",
    "    # Query the table as it was at that snapshot\n",
    "    historical_result = con.execute(\"\"\"\n",
    "        SELECT COUNT(*) as row_count\n",
    "        FROM seeknal_catalog.prod.customer_features\n",
    "    \"\"\").fetchone()\n",
    "    \n",
    "    print(f\"\\nRows as of snapshot {first_snapshot_id[:16]}...: {historical_result[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append vs Overwrite Modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new data for append\n",
    "new_data = pd.DataFrame({\n",
    "    \"customer_id\": [\"A004\", \"A005\"],\n",
    "    \"event_date\": [\"2024-01-03\", \"2024-01-03\"],\n",
    "    \"total_orders\": [7, 15],\n",
    "    \"total_spend\": [175.0, 375.0],\n",
    "    \"avg_order_value\": [25.0, 25.0],\n",
    "    \"days_since_last_order\": [0, 0],\n",
    "})\n",
    "\n",
    "# Append mode - adds new rows\n",
    "fg_append = FeatureGroup(\n",
    "    name=\"customer_features_append\",\n",
    "    entity=customer_entity,\n",
    "    materialization=Materialization(\n",
    "        event_time_col=\"event_date\",\n",
    "        offline=True,\n",
    "        offline_materialization=OfflineMaterialization(\n",
    "            store=OfflineStore(\n",
    "                kind=OfflineStoreEnum.ICEBERG,\n",
    "                value=IcebergStoreOutput(\n",
    "                    catalog=\"lakekeeper\",\n",
    "                    namespace=\"prod\",\n",
    "                    table=\"customer_features_append\",\n",
    "                    mode=\"append\"  # Append mode\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "fg_append.set_dataframe(features_df).set_features()\n",
    "result1 = fg_append.write(feature_start_time=datetime(2024, 1, 1))\n",
    "\n",
    "fg_append.set_dataframe(new_data).set_features()\n",
    "result2 = fg_append.write(feature_start_time=datetime(2024, 1, 3))\n",
    "\n",
    "print(f\"After first write: {result1['num_rows']} rows\")\n",
    "print(f\"After append: {result2['num_rows']} rows (should be >= {result1['num_rows']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Iceberg Features in ML Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query features for ML training\n",
    "training_data = con.execute(\"\"\"\n",
    "SELECT \n",
    "    customer_id,\n",
    "    total_orders,\n",
    "    total_spend,\n",
    "    avg_order_value,\n",
    "    days_since_last_order\n",
    "FROM seeknal_catalog.prod.customer_features\n",
    "WHERE event_date >= '2024-01-01'\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(\"\\nTraining data shape:\", training_data.shape)\n",
    "print(\"\\nTraining data preview:\")\n",
    "print(training_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use features with scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create a simple classification target\n",
    "training_data['high_value'] = (training_data['total_spend'] > 100).astype(int)\n",
    "\n",
    "# Split data\n",
    "X = training_data[['total_orders', 'avg_order_value', 'days_since_last_order']]\n",
    "y = training_data['high_value']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nModel accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced: Schema Evolution\n",
    "\n",
    "Iceberg allows schema evolution without rewriting data. Let's add a new feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data with a new column\n",
    "enhanced_data = pd.DataFrame({\n",
    "    \"customer_id\": [\"A001\", \"A002\"],\n",
    "    \"event_date\": [\"2024-01-04\", \"2024-01-04\"],\n",
    "    \"total_orders\": [8, 12],\n",
    "    \"total_spend\": [200.0, 300.0],\n",
    "    \"avg_order_value\": [25.0, 25.0],\n",
    "    \"days_since_last_order\": [0, 0],\n",
    "    \"customer_segment\": [\"VIP\", \"Regular\"],  # NEW COLUMN!\n",
    "})\n",
    "\n",
    "print(\"Data with new 'customer_segment' column:\")\n",
    "print(enhanced_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write with schema evolution\n",
    "# Note: In production, handle schema compatibility first\n",
    "print(\"\\nSchema evolution with Iceberg:\")\n",
    "print(\"  - New columns are automatically added\")\n",
    "print(\"  - Existing data gets NULL for new columns\")\n",
    "print(\"  - No table rewrite required\")\n",
    "print(\"\\nThis feature is powerful for:\")\n",
    "print(\"  - Adding new features over time\")\n",
    "print(\"  - A/B testing different feature sets\")\n",
    "print(\"  - Gradual schema migration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance tips for Iceberg Feature Groups:\n",
    "\n",
    "tips = \"\"\"\n",
    "1. PARTITIONING: Partition by date for efficient time-based queries\n",
    "   - Example: PARTITION BY days(event_date)\n",
    "\n",
    "2. ZORDERING: Sort by frequently filtered columns\n",
    "   - Example: ZORDER BY customer_id\n",
    "\n",
    "3. SNAPSHOTS: Manage snapshot retention\n",
    "   - Old snapshots can be expired to save metadata space\n",
    "\n",
    "4. FILE SIZE: Aim for 128-256MB parquet files\n",
    "   - Too small: metadata overhead\n",
    "   - Too large: slow reads\n",
    "\n",
    "5. CACHING: DuckDB caches Iceberg metadata\n",
    "   - First query is slower, subsequent queries are fast\n",
    "\"\"\"\n",
    "\n",
    "print(tips)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = \"\"\"\n",
    "Iceberg Feature Groups provide:\n",
    "\n",
    "✓ ACID transactions for reliable feature writes\n",
    "✓ Time travel for debugging and rollback\n",
    "✓ Schema evolution without data rewrites\n",
    "✓ Cloud-native storage with S3/GCS/Azure support\n",
    "✓ Compatible with DuckDB, Spark, Trino, and more\n",
    "✓ Point-in-time joins prevent data leakage\n",
    "\n",
    "Key Benefits:\n",
    "- Production-ready storage for feature groups\n",
    "- Team collaboration through shared catalog\n",
    "- Audit trail through snapshot history\n",
    "- Easy integration with existing ML pipelines\n",
    "\"\"\"\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- **Production Setup**: Deploy Lakekeeper and configure S3/GCS storage\n",
    "- **Feature Store Docs**: See `docs/getting-started-comprehensive.md`\n",
    "- **API Reference**: Check `docs/api/` for detailed API docs\n",
    "- **YAML Pipelines**: Use Iceberg in workflow YAML definitions\n",
    "\n",
    "For issues or questions:\n",
    "- GitHub: https://github.com/mta-tech/seeknal/issues"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
