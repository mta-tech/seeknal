{
  "spec": "specs/2026-02-21-feat-project-aware-repl-plan-feat-env-aware-iceberg-postgresql-plan-merged.md",
  "spec_title": "Project-Aware REPL + Environment-Aware Iceberg & PostgreSQL",
  "completed_tasks": 10,
  "extraction_date": "2026-02-21T00:00:00Z",

  "decisions": [
    {
      "type": "architecture",
      "title": "REPL three-phase best-effort auto-registration",
      "context": "Developers running seeknal repl inside a project directory had to manually issue .connect commands to inspect pipeline outputs. Three distinct data sources need registration: intermediate parquets, PostgreSQL connections, and Iceberg catalogs. Each has different failure modes (missing files, network timeouts, missing OAuth2 config).",
      "rationale": "Each registration phase is wrapped in an independent try/except so a failure in Phase 2 (PostgreSQL unreachable) does not block Phase 3 (Iceberg). Failures emit warnings rather than raising exceptions, preserving the REPL startup experience. Zero manual .connect steps on project entry.",
      "consequences": [
        "REPL startup may take a few extra seconds when PostgreSQL connections time out (mitigated by connect_timeout=5)",
        "Failed registrations are visible as warnings, not errors — developers see what is available vs what failed",
        "REPL outside a project directory is completely unaffected (no regression)",
        "Registration counts are tracked as instance attributes for the startup banner"
      ],
      "alternatives": [
        "Single try/except around all three phases — rejected because one failure would silence the other two",
        "Interactive prompts at startup asking what to register — rejected, reduces automation value",
        "Lazy registration on first .tables call — rejected, banner must show registration summary at startup"
      ],
      "evidence": ["task-2-repl", "task-3-repl", "task-9-repl"]
    },
    {
      "type": "architecture",
      "title": "ExecutionContext optional parameter with legacy fallback in DAGRunner",
      "context": "DAGRunner was previously invoked without an execution context by many callers (seeknal run, parallel mode, tests). Adding ExecutionContext support for the new env-aware executor path must not break any existing caller.",
      "rationale": "DAGRunner.__init__ accepts exec_context as Optional[ExecutionContext] defaulting to None. _execute_by_type checks if self.exec_context is not None and routes to the new get_executor() registry path; otherwise falls back to the legacy execute_* function dispatch. This makes the new path purely opt-in.",
      "consequences": [
        "All existing callers continue working without modification",
        "New env-aware path (seeknal env apply, seeknal run --env) passes an ExecutionContext",
        "TYPE_CHECKING import used for ExecutionContext to avoid circular imports at runtime",
        "Both paths coexist indefinitely until legacy path is deprecated"
      ],
      "alternatives": [
        "Require ExecutionContext for all callers — rejected, too many breaking changes at once",
        "Separate RunnerV2 class — rejected, duplicates too much logic"
      ],
      "evidence": ["task-4-env", "task-10-env"]
    },
    {
      "type": "architecture",
      "title": "profile_path round-trip through plan.json for env plan → env apply decoupling",
      "context": "seeknal env plan and seeknal env apply are separate commands that may run at different times and in different shells. The profile_path used during planning must be available at apply time without requiring the user to re-specify it.",
      "rationale": "EnvironmentPlan dataclass carries an optional profile_path field (stored as string in JSON, re-converted to Path on restore). plan() serializes it into plan.json via dataclasses.asdict(). apply() reads it back with plan_data.get('profile_path') and returns it in the result dict. _run_in_environment then picks it up if not explicitly overridden.",
      "consequences": [
        "Users run seeknal env plan dev --profile profiles-dev.yml once; seeknal env apply dev uses the same profile without re-specification",
        "Explicit --profile on apply still overrides the stored path",
        "plan.json is human-readable JSON — profile_path is visible and auditable",
        "None is stored as JSON null when no profile was used"
      ],
      "alternatives": [
        "Store profile_path in env_config.json — rejected, plan.json is the natural place (it covers plan-time decisions)",
        "Require --profile on every apply — rejected, poor UX and error-prone"
      ],
      "evidence": ["task-5-env", "task-10-env"]
    },
    {
      "type": "architecture",
      "title": "Convention-based namespace prefixing with {env}_ prefix in MaterializationDispatcher",
      "context": "When running a pipeline in an environment (dev, staging) without a per-env profile, materialization targets would write to the same production schemas/namespaces as the production run, causing data pollution.",
      "rationale": "MaterializationDispatcher.dispatch() accepts an optional env_name parameter. When set, _prefix_target() is called per target before routing. PostgreSQL: schema.table → {env}_schema.table. Iceberg: catalog.ns.table → catalog.{env}_ns.table. The prefix only applies to the schema/namespace component, not the catalog or table name, to keep tables identifiable in multi-env catalogs.",
      "consequences": [
        "Dev run of analytics.orders writes to dev_analytics.orders — completely isolated",
        "No per-env profile needed for basic isolation; per-env profile gives full source isolation too",
        "Schema/namespace auto-creation is triggered automatically before each write",
        "Original target dict is never mutated (copy-on-write via dict(target))"
      ],
      "alternatives": [
        "Require a per-env profile for any env run — rejected, too much config burden for quick dev testing",
        "Prefix the table name instead of schema — rejected, table names must stay consistent for lineage tracking",
        "Suffix instead of prefix — rejected, prefix makes schema listing (alphabetical sort) group env schemas together"
      ],
      "evidence": ["task-7-env", "task-10-env"]
    },
    {
      "type": "architecture",
      "title": "Per-env profile auto-discovery with four-tier priority chain",
      "context": "Users running seeknal run --env dev should automatically pick up dev-specific credentials without specifying --profile every time. A consistent convention reduces cognitive overhead.",
      "rationale": "_resolve_env_profile() implements a priority chain: (1) explicit --profile flag, (2) profiles-{env}.yml in project root, (3) ~/.seeknal/profiles-{env}.yml, (4) None (fall through to ProfileLoader's own default). The function is called from both _run_in_environment and env_plan, ensuring consistent behavior across all env-related commands.",
      "consequences": [
        "Users can place profiles-dev.yml in the project root for project-local dev credentials",
        "Home-level profiles-dev.yml works for users with consistent dev setups across multiple projects",
        "No --profile flag needed for the common case",
        "Explicit --profile still overrides everything for CI/CD or one-off runs"
      ],
      "alternatives": [
        "Environment variable SEEKNAL_DEV_PROFILE — rejected, less discoverable than file conventions",
        "Single profiles.yml with environment sections — rejected, changes the existing profiles.yml format"
      ],
      "evidence": ["task-6-env", "task-10-env"]
    },
    {
      "type": "implementation",
      "title": "RunState loaded from run_state.json for REPL startup banner metadata",
      "context": "The REPL banner should show project context including total node count and last run timestamp. This information is available in the existing run_state.json written by DAGRunner.",
      "rationale": "REPL._load_run_state() reads target/run_state.json with a simple json.load(), extracting len(data['nodes']) for node count and data['last_run'] for the timestamp. It is best-effort (wrapped in try/except) since the state file may not exist (first run). The last_run timestamp is truncated to 16 characters for display (YYYY-MM-DD HH:MM).",
      "consequences": [
        "Banner shows accurate node count from last run without re-parsing YAML",
        "Banner gracefully degrades: no run_state.json → no node count/last run shown",
        "No DAGBuilder or Manifest loading at REPL startup — fast initialization"
      ],
      "alternatives": [
        "Re-parse YAML pipeline files to count nodes — rejected, slow and fragile at startup",
        "Store a separate REPL metadata file — rejected, run_state.json already has all needed data"
      ],
      "evidence": ["task-3-repl", "task-9-repl"]
    },
    {
      "type": "implementation",
      "title": "PostgreSQL REPL attachment uses connect_timeout=5 in libpq string",
      "context": "When a PostgreSQL server in profiles.yml is unreachable, the DuckDB ATTACH command would block for the OS default TCP timeout (typically 30+ seconds), making REPL startup extremely slow.",
      "rationale": "The libpq connection string constructed in _register_postgresql_connections() includes connect_timeout=5 as a hard-coded parameter. This caps the per-connection wait at 5 seconds. A failed connection is silently skipped (pass in the except block), so slow servers impose at most 5 seconds of total startup delay.",
      "consequences": [
        "REPL startup delay bounded at 5s per unreachable PostgreSQL connection",
        "connect_timeout=5 is not user-configurable (hard-coded) to keep the API simple",
        "Successfully attached connections have the timeout param in the connection string but it is only used during connection establishment"
      ],
      "alternatives": [
        "Default OS TCP timeout (~30s) — rejected, unacceptably slow for REPL startup",
        "Configurable timeout in profiles.yml — rejected, over-engineering for a startup UX optimization"
      ],
      "evidence": ["task-2-repl", "task-9-repl"]
    },
    {
      "type": "implementation",
      "title": "Atomic promotion via copy-to-temp-then-rename with run_state.json as applied marker",
      "context": "Environment promotion must be both atomic (avoid partial state in production) and gated on the environment having been actually applied (not just planned).",
      "rationale": "promote() creates a temp dir alongside the production cache, copies the production cache there as a backup, then copies the env cache over production. The temp dir is always cleaned up in a finally block. A separate check for env_dir/run_state.json existence gates promotion — only applied environments (those that have executed) can be promoted.",
      "consequences": [
        "If promotion fails mid-copy, production cache is recoverable from the temp backup",
        "seeknal env apply writes run_state.json as a side effect, becoming the applied marker",
        "Promoting an environment that was only planned (never applied) raises a clear ValueError"
      ],
      "alternatives": [
        "Direct overwrite without temp backup — rejected, leaves production in partial state on failure",
        "Separate is_applied flag in env_config.json — rejected, run_state.json already serves this purpose"
      ],
      "evidence": ["task-8-env", "task-10-env"]
    }
  ],

  "errors": [
    {
      "type": "database",
      "category": "duckdb-type-compat",
      "symptom": "Iceberg materialization failed with type error when writing COUNT(*) or SUM() results: 'HUGEINT is not a valid Iceberg type'",
      "investigation_steps": [
        "DuckDB COUNT(*) and SUM() operations return HUGEINT (128-bit integer) by default",
        "Iceberg REST catalog rejected the schema because HUGEINT has no Iceberg type equivalent",
        "Checked DuckDB docs: COUNT(*) returns HUGEINT, not BIGINT, to avoid overflow on large tables",
        "Iceberg type spec: only INT, LONG (64-bit), FLOAT, DOUBLE are valid numeric non-decimal types"
      ],
      "root_cause": "DuckDB's aggregate functions COUNT(*) and SUM() produce HUGEINT (128-bit integer) columns by default. The Iceberg type system has no HUGEINT equivalent — only INT (32-bit) and LONG/BIGINT (64-bit). When DuckDB writes schema to the Iceberg REST catalog, the HUGEINT column type is rejected.",
      "solution": "Wrap aggregate expressions with explicit CAST: CAST(COUNT(*) AS BIGINT) and CAST(SUM(col) AS DOUBLE) instead of DECIMAL. Apply the cast at the SELECT level in the transform SQL, or in the final view that is passed to write_to_iceberg().",
      "result": "Iceberg materialization succeeded with correctly typed schemas. BIGINT and DOUBLE are valid Iceberg types that round-trip correctly.",
      "prevention_strategies": [
        "Always CAST(COUNT(*) AS BIGINT) and CAST(SUM(...) AS BIGINT or DOUBLE) in transforms that write to Iceberg",
        "Add a schema validation step before Iceberg write that detects HUGEINT columns and raises a clear error with the fix",
        "Document this in the Iceberg materialization guide: 'DuckDB aggregate types must be explicitly cast'",
        "QA test all aggregation nodes in the medallion pipeline against Iceberg targets to catch this early"
      ],
      "test_cases": [
        "test_iceberg_count_star_cast_bigint",
        "test_python_iceberg_medallion_gold_aggregations"
      ],
      "evidence": ["qa-iceberg-medallion", "qa-python-iceberg-medallion"]
    },
    {
      "type": "database",
      "category": "duckdb-sql",
      "symptom": "DuckDB error 'column must appear in GROUP BY clause or be used in aggregate' when writing aggregation results to Iceberg",
      "investigation_steps": [
        "Gold aggregation node selected non-aggregated columns alongside aggregate expressions without listing them in GROUP BY",
        "DuckDB enforces SQL standard GROUP BY requirements strictly (unlike SQLite which permits this)",
        "The transform SQL was ported from a permissive SQL dialect and assumed all SELECT columns were implicitly grouped"
      ],
      "root_cause": "DuckDB requires that all non-aggregate columns in a SELECT with any aggregate function must appear in the GROUP BY clause. This is standard SQL behavior. Transforms ported from permissive SQL dialects (SQLite, MySQL in some modes) may omit GROUP BY for columns implicitly determined by the grouped key.",
      "solution": "Add all non-aggregated columns to the GROUP BY clause in the transform SQL. Verify each SELECT expression: if it is not wrapped in an aggregate function (COUNT, SUM, MIN, MAX, AVG), it must appear in GROUP BY.",
      "result": "Aggregation transforms ran successfully once GROUP BY was complete.",
      "prevention_strategies": [
        "Use DuckDB for local development/testing to catch GROUP BY issues before CI",
        "Code review checklist: for every SELECT with aggregate functions, verify all non-aggregate columns are in GROUP BY",
        "Consider a static SQL linter step in the pipeline validation phase"
      ],
      "test_cases": [
        "test_aggregation_group_by_completeness"
      ],
      "evidence": ["qa-iceberg-medallion", "qa-python-iceberg-medallion"]
    },
    {
      "type": "integration",
      "category": "profile-loading",
      "symptom": "source_defaults alias normalization mismatch: postgres sources looked up as 'postgresql' in source_defaults but profiles.yml used 'postgres' as the key, causing silent no-op (no defaults merged)",
      "investigation_steps": [
        "source_defaults section in profiles.yml used 'postgres' key (not canonical 'postgresql')",
        "DAGBuilder._merge_source_defaults() looked up source type 'postgresql' (after normalization)",
        "Lookup missed because key was 'postgres' not 'postgresql' — no error, just no defaults applied",
        "QA test showed source node got no connection injected despite source_defaults being present"
      ],
      "root_cause": "ProfileLoader.load_source_defaults() normalizes the source_type argument (postgres→postgresql) before looking up in the YAML dict. However, the dict keys themselves are NOT normalized — if the user wrote 'postgres:' in profiles.yml, the normalized lookup 'postgresql' finds nothing. The fix requires either normalizing keys at load time or accepting both.",
      "solution": "In DAGBuilder._merge_source_defaults() or ProfileLoader.load_source_defaults(), normalize the profile YAML keys as well: try both 'postgresql' and 'postgres' as lookup keys, preferring the canonical form. Alternatively, normalize keys when loading source_defaults from profiles.yml.",
      "result": "After fix: source nodes with source: postgres or source: postgresql both correctly receive defaults from either 'postgres:' or 'postgresql:' keys in profiles.yml.",
      "prevention_strategies": [
        "Always normalize both the lookup key AND the dict keys in source_defaults resolution",
        "Add a QA test that uses 'postgres' as the key in profiles.yml and verifies defaults are applied",
        "Document that both 'postgres' and 'postgresql' are valid keys in source_defaults"
      ],
      "test_cases": [
        "test_postgres_alias_resolves (tests/workflow/test_dag_source_defaults.py)"
      ],
      "evidence": ["task-5-env", "qa-spec-interpreter", "known-bugs-memory"]
    },
    {
      "type": "integration",
      "category": "source-config",
      "symptom": "Source executor rejected node config with both 'table:' and 'params.query:' present, even when table was inherited from source_defaults and query was the intended override",
      "investigation_steps": [
        "Source executor validates that table and query are mutually exclusive (pushdown query replaces table scan)",
        "source_defaults injected a 'table' param from defaults",
        "User's inline 'query:' was meant to override, but the validator saw both present simultaneously",
        "Result: misleading 'cannot specify both table and query' error"
      ],
      "root_cause": "The merge order in source_defaults processing injected 'table' from defaults even when the node YAML had an explicit 'query' param. Defaults should not inject fields that conflict with explicit inline params.",
      "solution": "In _merge_source_defaults(), when merging defaults into node params, skip any default key that conflicts with an already-present explicit param. Specifically: if the node already has 'query' in its params, do not inject 'table' from defaults.",
      "result": "Source nodes with explicit pushdown queries receive connection credentials from defaults but not conflicting table param.",
      "prevention_strategies": [
        "source_defaults merge must be 'additive but non-conflicting': defaults fill missing fields, never overwrite explicit ones",
        "Add explicit test: source with both 'query:' inline and 'table:' in source_defaults → only query used"
      ],
      "test_cases": [
        "test_pushdown_query_table_mutual_exclusivity"
      ],
      "evidence": ["known-bugs-memory", "qa-spec-interpreter"]
    }
  ],

  "deployment": {
    "environment": "development",
    "platform": "darwin (macOS)",
    "changes": [
      {
        "type": "config-convention",
        "description": "Per-environment profiles.yml convention: profiles-{env}.yml",
        "details": "seeknal now auto-discovers profiles-dev.yml, profiles-staging.yml, etc. in the project root or ~/.seeknal/. Users can place env-specific credentials in these files without modifying the default profiles.yml."
      },
      {
        "type": "filesystem",
        "description": "Environment cache and state directory structure under target/environments/{env}/",
        "details": "Each virtual environment stores: plan.json (plan + profile_path), manifest.json (snapshot), env_config.json (TTL metadata), refs.json (production references for unchanged nodes), cache/ (env-specific parquet outputs), run_state.json (applied marker for promote gating)"
      },
      {
        "type": "runtime",
        "description": "REPL reads run_state.json and target/cache/**/*.parquet at startup when seeknal_project.yml detected",
        "details": "New REPL startup reads target/run_state.json for banner metadata and target/cache/ for parquet registration. No new persistent files created by REPL startup."
      }
    ]
  },

  "patterns": [
    {
      "name": "REPL three-phase best-effort registration",
      "pattern": "Wrap each registration phase in an independent try/except that emits warnings on failure. Never let one phase failure block subsequent phases.",
      "correct": "def _auto_register_project(self) -> None:\n    import warnings\n    try:\n        self._register_parquets()\n    except Exception as e:\n        warnings.warn(f'REPL: Failed to register parquets: {e}')\n    try:\n        self._register_postgresql_connections()\n    except Exception as e:\n        warnings.warn(f'REPL: Failed to attach PostgreSQL connections: {e}')\n    try:\n        self._register_iceberg_catalogs()\n    except Exception as e:\n        warnings.warn(f'REPL: Failed to attach Iceberg catalogs: {e}')",
      "incorrect": "def _auto_register_project(self) -> None:\n    try:\n        self._register_parquets()\n        self._register_postgresql_connections()  # If this fails...\n        self._register_iceberg_catalogs()        # ...this never runs\n    except Exception as e:\n        warnings.warn(f'REPL: Registration failed: {e}')",
      "rationale": "REPL startup must degrade gracefully. A PostgreSQL connection that times out or an Iceberg catalog that is unreachable should not prevent intermediate parquet files from being queryable. Independent try/except per phase ensures maximum availability.",
      "category": "implementation"
    },
    {
      "name": "Optional ExecutionContext with legacy fallback in DAGRunner",
      "pattern": "Accept exec_context as Optional and check is not None before routing to the new executor path. This preserves backward compatibility without requiring all callers to be updated simultaneously.",
      "correct": "def _execute_by_type(self, node: Node) -> Dict[str, Any]:\n    if self.exec_context is not None:\n        from seeknal.workflow.executors import get_executor\n        executor = get_executor(node, self.exec_context)\n        result = executor.run()\n        return {'row_count': result.row_count, 'status': result.status.value}\n    # Legacy path: use execute_* functions\n    from seeknal.workflow.executor import execute_source, execute_transform\n    ...",
      "incorrect": "def _execute_by_type(self, node: Node) -> Dict[str, Any]:\n    # Always use new path — breaks all existing callers without exec_context\n    from seeknal.workflow.executors import get_executor\n    executor = get_executor(node, self.exec_context)  # AttributeError: exec_context is None",
      "rationale": "Large systems cannot migrate all call sites simultaneously. The Optional+fallback pattern allows the new path to be opt-in while keeping the old path functional until explicit migration is planned.",
      "category": "architecture"
    },
    {
      "name": "profile_path round-trip persistence in plan.json",
      "pattern": "When a workflow has a plan/apply split across separate commands, persist all plan-time parameters (including file paths) in the plan JSON. Restore them at apply time as the default, allowing explicit override.",
      "correct": "# plan() — EnvironmentManager\nplan = EnvironmentPlan(\n    ...\n    profile_path=str(profile_path) if profile_path else None,\n)\nself._save_json(env_dir / 'plan.json', asdict(plan))\n\n# apply() — EnvironmentManager\nsaved_profile = plan_data.get('profile_path')\nprofile_path = Path(saved_profile) if saved_profile else None\nreturn {'profile_path': profile_path, ...}\n\n# _run_in_environment() — CLI\nif profile_path is None and apply_info.get('profile_path'):\n    profile_path = apply_info['profile_path']",
      "incorrect": "# Forgetting to persist profile_path forces users to re-specify on every apply\n# CLI: seeknal env apply dev --profile profiles-dev.yml  (annoying, error-prone)",
      "rationale": "Plan-time decisions should be durable. The user made a decision (which profile to use) at plan time; that decision should survive to apply time without requiring re-specification. Explicit override is still possible.",
      "category": "implementation"
    },
    {
      "name": "Convention-based namespace prefixing for environment isolation",
      "pattern": "When materializing in a non-production environment, prefix the schema/namespace with the environment name ({env}_) to isolate writes. Copy the target dict before modifying to avoid mutation.",
      "correct": "@staticmethod\ndef _prefix_target(target: Dict[str, Any], env_name: str) -> Dict[str, Any]:\n    prefixed = dict(target)  # Copy, never mutate original\n    table = prefixed.get('table', '')\n    target_type = prefixed.get('type', 'iceberg')\n    if target_type == 'postgresql' and '.' in table:\n        schema, table_name = table.split('.', 1)\n        prefixed['table'] = f'{env_name}_{schema}.{table_name}'\n    elif target_type == 'iceberg' and table.count('.') >= 2:\n        parts = table.split('.', 2)\n        prefixed['table'] = f'{parts[0]}.{env_name}_{parts[1]}.{parts[2]}'\n    return prefixed",
      "incorrect": "# Mutating the original target dict causes the production table name to be\n# permanently modified for the lifetime of the object\ntarget['table'] = f'{env_name}_{target[\"table\"]}'  # Wrong: mutates caller's dict",
      "rationale": "Environment isolation prevents dev/staging runs from polluting production schemas. Convention-based prefixing (no per-env profile required) makes isolation the default for any --env run. Copy-on-write prevents subtle bugs from dict mutation across multiple dispatch calls.",
      "category": "architecture"
    },
    {
      "name": "Per-env profile auto-discovery with explicit > project > home > default priority",
      "pattern": "Resolve profile paths through a priority chain: explicit flag beats project-local convention beats home-dir convention beats default. Return None (not a default path) when nothing is found, letting downstream components apply their own defaults.",
      "correct": "def _resolve_env_profile(env_name, project_path, explicit_profile=None):\n    if explicit_profile is not None:\n        return explicit_profile\n    project_profile = project_path / f'profiles-{env_name}.yml'\n    if project_profile.exists():\n        return project_profile\n    from seeknal.context import CONFIG_BASE_URL\n    home_profile = Path(CONFIG_BASE_URL) / f'profiles-{env_name}.yml'\n    if home_profile.exists():\n        return home_profile\n    return None  # Let ProfileLoader use its own default",
      "incorrect": "def _resolve_env_profile(env_name, project_path):\n    # Returns hardcoded default — cannot be overridden\n    return Path.home() / '.seeknal' / 'profiles.yml'",
      "rationale": "Returning None preserves ProfileLoader's ability to apply its own default (profiles.yml in the home dir). Hardcoding a fallback path ties the resolution logic to ProfileLoader internals and prevents clean separation of concerns.",
      "category": "implementation"
    },
    {
      "name": "PostgreSQL read-only ATTACH with short connect_timeout for REPL",
      "pattern": "When attaching PostgreSQL connections in an interactive tool, always include connect_timeout=5 in the libpq string and READ_ONLY in the ATTACH statement to prevent credential errors and slow startup.",
      "correct": "conn_str = (\n    f'host={host} port={port} dbname={database} '\n    f'user={user} password={password} '\n    f'connect_timeout=5'\n)\nself.conn.execute(\n    f\"ATTACH '{conn_str}' AS \\\"{name}\\\" (TYPE postgres, READ_ONLY)\"\n)",
      "incorrect": "# No timeout — hangs for OS default TCP timeout (30s+) on unreachable hosts\n# No READ_ONLY — allows accidental writes from REPL queries\nself.conn.execute(\n    f\"ATTACH '{conn_str}' AS {name} (TYPE postgres)\"\n)",
      "rationale": "REPL is for exploration — READ_ONLY prevents accidental writes. connect_timeout=5 caps startup delay when a server is unreachable. Combined, they make REPL startup reliable and bounded in time.",
      "category": "implementation"
    },
    {
      "name": "DuckDB HUGEINT to BIGINT cast for Iceberg compatibility",
      "pattern": "When writing DuckDB aggregate results (COUNT, SUM) to Iceberg, always CAST to BIGINT or DOUBLE. DuckDB returns HUGEINT for aggregates by default, which has no Iceberg type equivalent.",
      "correct": "SELECT\n    category,\n    CAST(COUNT(*) AS BIGINT) AS order_count,\n    CAST(SUM(amount) AS DOUBLE) AS total_amount\nFROM orders\nGROUP BY category",
      "incorrect": "SELECT\n    category,\n    COUNT(*) AS order_count,    -- HUGEINT -- not supported by Iceberg\n    SUM(amount) AS total_amount  -- HUGEINT -- not supported by Iceberg\nFROM orders\nGROUP BY category",
      "rationale": "Iceberg type spec supports only INT (32-bit) and LONG/BIGINT (64-bit) integers — no 128-bit HUGEINT. DuckDB's COUNT(*) and SUM() default to HUGEINT to avoid overflow on large tables. Explicit CAST is required when the output schema must be Iceberg-compatible.",
      "category": "database"
    },
    {
      "name": "GROUP BY completeness enforcement for DuckDB aggregations",
      "pattern": "In every SELECT with at least one aggregate function, all non-aggregate columns must appear in GROUP BY. DuckDB enforces this strictly — test with DuckDB locally to catch issues before writing to Iceberg.",
      "correct": "SELECT\n    region,\n    product_category,\n    CAST(COUNT(*) AS BIGINT) AS order_count\nFROM orders\nGROUP BY region, product_category  -- All non-aggregate columns listed",
      "incorrect": "SELECT\n    region,\n    product_category,  -- Missing from GROUP BY!\n    CAST(COUNT(*) AS BIGINT) AS order_count\nFROM orders\nGROUP BY region  -- Incomplete GROUP BY",
      "rationale": "DuckDB follows strict SQL standard GROUP BY rules. Transforms ported from permissive dialects (SQLite, MySQL with ONLY_FULL_GROUP_BY off) will fail with 'must appear in GROUP BY' errors. The rule is: if any column has an aggregate, all non-aggregate columns must be in GROUP BY.",
      "category": "database"
    },
    {
      "name": "Schema validation before injection in source_defaults merge",
      "pattern": "When merging source_defaults into node params, skip any default key that conflicts with an already-present explicit param. Defaults fill missing fields, never overwrite explicit ones.",
      "correct": "def _merge_source_defaults(self, node_yaml, defaults):\n    existing_params = node_yaml.get('params') or {}\n    for key, value in defaults.items():\n        if key not in existing_params:  # Only inject if not already set\n            existing_params[key] = value\n    node_yaml['params'] = existing_params",
      "incorrect": "def _merge_source_defaults(self, node_yaml, defaults):\n    existing_params = node_yaml.get('params') or {}\n    merged = {**defaults, **existing_params}  # Almost right...\n    # But if defaults has 'table' and user has 'query', merged still has 'table'\n    # because defaults fills it in before existing_params can override\n    node_yaml['params'] = merged",
      "rationale": "source_defaults are intended to reduce boilerplate (inject shared credentials/catalog URIs) not override explicit user intent. A node with an explicit 'query:' param should never receive a 'table:' from defaults, as table and query are mutually exclusive in source validation.",
      "category": "implementation"
    }
  ]
}
